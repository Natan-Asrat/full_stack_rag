{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install langchain_groq\n",
        "!pip install unstructured\n",
        "!pip install pdfminer.six\n",
        "!pip install langchain\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z9zv-M1s0b1",
        "outputId": "62d43ba9-7e0c-4253-af25-9c4df7788499"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.0)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.6)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.6.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.5)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.8.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.11.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.3.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (0.1.128)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain_groq) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.2.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.128)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.128)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fVd5NlZxjeYR"
      },
      "outputs": [],
      "source": [
        "# from dotenv import load_dotenv\n",
        "import os\n",
        "# load_dotenv()\n",
        "os.environ[\"GROQ_API_KEY\"]=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "\n",
        "web_loader = WebBaseLoader(web_paths=(\"https://en.wikipedia.org/wiki/Robotics\",),\n",
        "                           bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
        "                               class_=(\"mw-body-content\")\n",
        "                           )))\n",
        "web_documents = web_loader.load()\n",
        "print(web_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03cAy6sXjuq5",
        "outputId": "1df01570-9e75-496f-c602-c1e7ad70c887"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Design, construction, use, and application of robots\\nThis article may relate to a different subject or has undue weight on an aspect of the subject. Specifically, the article goes in too much detail on specific types of robot and includes product placement. Please help relocate relevant information and remove irrelevant content. (August 2024)\\n\\n\\nRoboticists with three Mars rover robots. Front and center is the flight spare for the first Mars rover, Sojourner, which landed on Mars in 1997 as part of the Mars Pathfinder Project. On the left is a Mars Exploration Rover (MER) test vehicle that is a working sibling to Spirit and Opportunity, which landed on Mars in 2004. On the right is a test rover for the Mars Science Laboratory, which landed Curiosity on Mars in 2012.\\n\\nRobotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]\\nWithin mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. \\n\\nThe goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\\n\\n\\n\\nRobotics aspects[edit]\\nMechanical aspect\\nElectrical aspect\\nSoftware aspect\\nRobotics usually combines three aspects of design work to create robot systems:\\n\\nMechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments.[2] The mechanical aspect of the robot is mostly the creator\\'s solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function.\\nElectrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol-powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status), and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations)\\nSoftware. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not be able to go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly structured, its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence, and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. A hybrid is a form of programming that incorporates both AI and RC functions in them.[3]\\nApplied robotics[edit]\\nAs more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4]\\nCurrent and potential applications include:\\n\\nManufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales.[5] In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[6]\\nAutonomous transport including airplane autopilot and self-driving cars\\nDomestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading[7] and flatbread baking.[8]\\nConstruction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[9]\\nAutomated mining.\\nSpace exploration, including Mars rovers.\\nEnergy applications including cleanup of nuclear contaminated areas[a]; and cleaning solar panel arrays.\\nMedical robots and Robot-assisted surgery designed and used in clinics.[11]\\nAgricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13]\\nFood processing. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts), Sally (salads),[14] salad or food bowl robots manufactured by Dexai (a Draper Laboratory spinoff, operating on military bases), and integrated food bowl assembly systems manufactured by Spyce Kitchen (acquired by Sweetgreen) and Silicon Valley startup Hyphen.[15] Other examples may include manufacturing technologies based on 3D Food Printing.\\nMilitary robots.\\nRobot sports for entertainment and education, including Robot combat, Autonomous racing, drone racing, and FIRST Robotics.\\nMechanical robotics areas[edit]\\nPower source[edit]\\nFurther information: Power supply and Energy storage\\nThe InSight lander with solar panels deployed in a cleanroom\\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] \\nPotential power sources could be:\\n\\npneumatic (compressed gases)\\nSolar power (using the sun\\'s energy and converting it into electrical power)\\nhydraulics (liquids)\\nflywheel energy storage\\norganic garbage (through anaerobic digestion)\\nnuclear\\nActuation[edit]\\nMain article: Actuator\\nA robotic leg powered by air muscles \\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\\n\\nElectric motors[edit]\\nMain article: Electric motor\\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\\n\\nLinear actuators[edit]\\nMain article: Linear actuator\\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.\\n\\nSeries elastic actuators[edit]\\nSeries elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21]\\nThe controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\\n\\nAir muscles[edit]\\nMain article: Pneumatic artificial muscles\\nPneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27]\\n\\nWire muscles[edit]\\nMain article: Shape memory alloy\\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29]\\n\\nElectroactive polymers[edit]\\nMain article: Electroactive polymers\\nEAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32]\\n\\nPiezo motors[edit]\\nMain article: Piezoelectric motor\\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[34] These motors are already available commercially and being used on some robots.[35][36]\\n\\nElastic nanotubes[edit]\\nFurther information: Carbon nanotube\\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10\\xa0J/cm3 for metal nanotubes. Human biceps could be replaced with an 8\\xa0mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37]\\n\\nSensing[edit]\\nMain articles: Robotic sensing and Robotic sensors\\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.\\n\\nTouch[edit]\\nMain article: Tactile sensor\\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[38][39] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40]\\n\\nFurther information: Sensory-motor map\\nOther[edit]\\nOther common forms of sensing in robotics use lidar, radar, and sonar.[41] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\\n\\nMechanical grippers[edit]\\nOne of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[43] Hands that are of a mid-level complexity include the Delft hand.[44][45] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\\n\\nSuction end-effectors[edit]\\nSuction end-effectors, powered by vacuum generators, are very simple astrictive[46] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\\nSuction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\\n\\nGeneral purpose effectors[edit]\\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47] and the Schunk hand.[48] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[49]\\n\\n\\nControl robotics areas[edit]\\nPuppet Magnus, a robot-manipulated marionette with complex control systems\\nExperimental planar robot arm and sensor-based, open-architecture robot controller\\nRuBot II can manually resolve Rubik\\'s cubes.\\nFurther information: Control system and Principles of motion sensing\\nThe mechanical structure of a robot must be controlled to perform tasks.[50] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[51] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot\\'s gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52]\\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\\nModern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[53] Progress towards open architecture, layered, user-friendly and \\'intelligent\\' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several \\'open or \\'hybrid\\' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of \\'closed\\' robot control systems have been proposed.[52] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[52] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55]\\n\\nManipulation[edit]\\nKUKA industrial robot operating in a foundry\\nPuma, one of the first industrial robots\\nBaxter, a modern and versatile industrial robot developed by Rodney Brooks\\nLefty, first checker playing robot\\nFurther information: Mobile manipulator\\nA definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent\\'s control of its environment through selective contact\".[56]\\nRobots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[57] while the \"arm\" is referred to as a manipulator.[58] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59]\\n\\n\\nLocomotion[edit]\\nMain articles: Robot locomotion and Mobile robot\\nRolling robots[edit]\\nSegway in the Robot museum in Nagoya\\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\\n\\nTwo-wheeled balancing robots[edit]\\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[60] Many different balancing robots have been designed.[61] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA\\'s Robonaut that has been mounted on a Segway.[62]\\n\\nOne-wheeled balancing robots[edit]\\nMain article: Self-balancing unicycle\\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University\\'s \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University\\'s \"BallIP\".[63] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64]\\n\\nSpherical orb robots[edit]\\nMain article: Spherical robot\\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66] or by rotating the outer shells of the sphere.[67][68] These have also been referred to as an orb bot[69] or a ball bot.[70][71]\\n\\nSix-wheeled robots[edit]\\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\\n\\nTracked robots[edit]\\nTracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA\\'s Urban Robot \"Urbie\".[72]\\n\\nWalking robots[edit]\\nFurther information: Mantis the spider robot\\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\\n\\nZMP technique[edit]\\nMain article: Zero moment point\\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda\\'s ASIMO. The robot\\'s onboard computer tries to keep the total inertial forces (the combination of Earth\\'s gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot\\'s foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[76] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[77][78][79] ASIMO\\'s walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\\n\\nHopping[edit]\\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[81] A quadruped was also demonstrated which could trot, run, pace, and bound.[82] For a full list of these robots, see the MIT Leg Lab Robots page.[83]\\n\\nDynamic balancing (controlled falling)[edit]\\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot\\'s motion, and places the feet in order to maintain stability.[84] This technique was recently demonstrated by Anybots\\' Dexter Robot,[85] which is so stable, it can even jump.[86] Another example is the TU Delft Flame.\\n\\nPassive dynamics[edit]\\nMain article: Passive dynamics\\nPerhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88]\\n\\n\\n\\n\\nFlying[edit]\\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.\\n\\nBiomimetic flying robots (BFRs)[edit]\\nA flapping wing BFR generating lift and thrust.\\nBFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[90] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\\nMammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91] Examples of bat inspired BFRs include Bat Bot[92] and the DALER.[93] Mammal inspired BFRs can be designed to be multi-modal; therefore, they\\'re capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91]\\n\\nDragonfly inspired BFR.\\nBird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94] An example of a raptor inspired BFR is the prototype by Savastano et al.[95] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8\\xa0kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96]\\nInsect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97] and a dragonfly inspired BFR is the prototype by Hu et al.[98] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[99] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.\\n\\nBiologically-inspired flying robots[edit]\\nVisualization of entomopter flying on Mars (NASA)\\nA class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.\\n\\nSnaking[edit]\\nTwo robot snakes. The left one has 64 motors (with 2 degrees of freedom per segment), the right one 10.\\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100] The Japanese ACM-R5 snake robot[101] can even navigate both on land and in water.[102]\\n\\nSkating[edit]\\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104]\\n\\nCapuchin, a climbing robot\\nClimbing[edit]\\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[105] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106] and Stickybot.[107]\\nChina\\'s Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41]\\n\\nSwimming (Piscine)[edit]\\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[108] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109] Notable examples are the Robotic Fish G9,[110] and Robot Tuna built to analyze and mathematically model thunniform motion.[111] The Aqua Penguin,[112] copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively.\\n\\n\\nRobotic Fish: iSplash-II\\nIn 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113] This build attained swimming speeds of 11.6BL/s (i.e. 3.7\\xa0m/s).[114] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115]\\n\\nSailing[edit]\\nThe autonomous sailboat robot Vaimos\\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos.[116] Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\\n\\nComputational robotics areas[edit]\\nTOPIO, a humanoid robot, played ping pong at Tokyo IREX 2009.[117]\\nControl systems may also have varying levels of autonomy.\\n\\nDirect interaction is used for haptic or teleoperated devices, and the human has nearly complete control over the robot\\'s motion.\\nOperator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them.[118]\\nAn autonomous robot may go without human interaction for extended periods of time . Higher levels of autonomy do not necessarily require more complex cognitive capabilities. For example, robots in assembly plants are completely autonomous but operate in a fixed pattern.\\nAnother classification takes into account the interaction between human control and the machine motions.\\n\\nTeleoperation. A human controls each movement, each machine actuator change is specified by the operator.\\nSupervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators.\\nTask-level autonomy. The operator specifies only the task and the robot manages itself to complete it.\\nFull autonomy. The machine will create and complete all its tasks without human interaction.\\n\\n\\nVision[edit]\\nMain article: Computer vision\\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\\nComputer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots\\' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.\\n\\nEnvironmental interaction and navigation[edit]\\nMain articles: Robotic mapping and Robotic navigation\\nRadar, GPS, and lidar, are all combined to provide proper navigation and obstacle avoidance (vehicle developed for 2007 DARPA Urban Challenge).\\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2009) (Learn how and when to remove this message)\\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns\\' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\\n\\nHuman-robot interaction[edit]\\nMain article: Human-robot interaction\\nKismet can produce a range of facial expressions.\\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people\\'s willingness to accept actual robots in the future.[120] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121]\\n\\nSpeech recognition[edit]\\nMain article: Speech recognition\\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[122] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[123] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125] With the help of artificial intelligence, machines nowadays can use people\\'s voice to identify their emotions such as satisfied or angry.[126]\\n\\nRobotic voice[edit]\\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[127] making it necessary to develop the emotional component of robotic voice through various techniques.[128][129] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[130][131] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132] It was programmed to teach students in The Bronx, New York.[132]\\n\\nFacial expression[edit]\\nFurther information: Emotion recognition\\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[134] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135]\\n\\nGestures[edit]\\nFurther information: Gesture recognition\\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136] A great many systems have been developed to recognize human hand gestures.[137]\\n\\nProxemics[edit]\\nProxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\\n\\nArtificial emotions[edit]\\nArtificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138]\\n\\nPersonality[edit]\\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[139] Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142]\\n\\nResearch robotics[edit]\\nFurther information: Areas of robotics\\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT\\'s cyberflora project, are almost wholly academic.\\nTo describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143]\\n\\nDynamics and kinematics[edit]\\nFurther information: Kinematics and Dynamics (mechanics)\\nExternal videos How the BB-8 Sphero Toy Works\\nThe study of motion can be divided into kinematics and dynamics.[144] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\\n\\nOpen source robotics[edit]\\nFurther information: Open source robotics\\nOpen source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.\\n\\nEvolutionary robotics[edit]\\nEvolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145] and to explore the nature of evolution.[146] Because the process often requires many generations of robots to be simulated,[147] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[148] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]\\n\\nBionics and biomimetics[edit]\\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\\n\\nSwarm robotics[edit]\\nSwarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [119]\\n\\nQuantum computing[edit]\\nThere has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[149]\\n\\nOther research areas[edit]\\nNanorobots.\\nCobots (collaborative robots).[150]\\nAutonomous drones.\\nHigh temperature crucibles allow robotic systems to automate sample analysis.[151]\\nThe main venues for robotics research are the international conferences ICRA and IROS.\\n\\nHuman factors[edit]\\nEducation and training[edit]\\nMain article: Educational robotics\\nThe SCORBOT-ER 4u educational robot\\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[152] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[153] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\\n\\nEmployment[edit]\\nA robot technician builds small all-terrain robots (courtesy: MobileRobots, Inc.).\\nMain article: Technological unemployment\\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[154] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[155] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[156] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[157]   The rise of robotics is thus often used as an argument for universal basic income.\\nAccording to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[158]\\n\\nOccupational safety and health implications[edit]\\nMain article: Workplace robotics safety\\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[159]\\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers\\' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[160]\\nMoreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\\nIn the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[161][162] aiming to protect employees from the risk of working with collaborative robots will have to be revised.\\n\\nUser experience[edit]\\nGreat user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot\\'s intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[163]\\nIt defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[164] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\\n\\nCareers[edit]\\nRobotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters\\') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.    \\nRobotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as \\'roboticists\\'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\\nRobotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.\\n\\nHistory[edit]\\nSee also: History of robots\\n\\n\\nDate\\n\\nSignificance\\n\\nRobot name\\n\\nInventor\\n\\n\\nc. 420 B.C.\\n\\nA wooden, steam-propelled bird, which was able to fly\\n\\nFlying pigeon\\n\\nArchytas of Tarentum\\n\\n\\nThird century B.C. and earlier\\n\\nOne of the earliest descriptions of automata appears in the Lie Zi text, on a much earlier encounter between King Mu of Zhou (1023–957 BC) and a mechanical engineer known as Yan Shi, an \\'artificer\\'. The latter allegedly presented the king with a life-size, human-shaped figure of his mechanical handiwork.[165]\\n\\n\\n\\nYan Shi (Chinese: 偃师)\\n\\n\\nFirst century A.D. and earlier\\n\\nDescriptions of more than 100 machines and automata, including a fire engine, a wind organ, a coin-operated machine, and a steam-powered engine, in Pneumatica and Automata by Heron of Alexandria\\n\\n\\n\\nCtesibius, Philo of Byzantium, Heron of Alexandria, and others\\n\\n\\n1206\\n\\nCreated early humanoid automata, programmable automaton band[166]Robot band, hand-washing automaton,[167] automated moving peacocks[168]\\n\\n\\n\\nAl-Jazari\\n\\n\\n1495\\n\\nDesigns for a humanoid robot\\n\\nMechanical Knight\\n\\nLeonardo da Vinci\\n\\n\\n1560s\\n\\nClockwork Prayer that had machinal feet built under its robes that imitated walking. The robot\\'s eyes, lips, and head all move in lifelike gestures.\\n\\nClockwork Prayer[citation needed]\\n\\nGianello della Torre\\n\\n\\n1738\\n\\nMechanical duck that was able to eat, flap its wings, and excrete\\n\\nDigesting Duck\\n\\nJacques de Vaucanson\\n\\n\\n1898\\n\\nNikola Tesla demonstrates the first radio-controlled vessel.\\n\\nTeleautomaton\\n\\nNikola Tesla\\n\\n\\n1903\\n\\nLeonardo Torres Quevedo presented the Telekino at the Paris Academy of Science, a radio-based control system with different operational states, for testing airships without risking human lives.[169] He conduct the initial test controlling a tricycle almost 100 feet away, being the first example of a radio-controlled unmanned ground vehicle.[170][171]\\n\\nTelekino\\n\\nLeonardo Torres Quevedo\\n\\n\\n1912\\n\\nLeonardo Torres Quevedo builds the first truly autonomous machine capable of playing chess. As opposed to the human-operated The Turk and Ajeeb, El Ajedrecista had an integrated automaton built to play chess without human guidance. It only played an endgame with three chess pieces, automatically moving a white king and a rook to checkmate the black king moved by a human opponent.[172][173]\\n\\nEl Ajedrecista\\n\\nLeonardo Torres Quevedo\\n\\n\\n1914\\n\\nIn his paper Essays on Automatics published in 1914, Leonardo Torres Quevedo proposed a machine that makes \"judgments\" using sensors that capture information from the outside, parts that manipulate the outside world like arms, power sources such as batteries and air pressure, and most importantly, captured information and past information. It was defined as an organism that can control reactions in response to external information and adapt to changes in the environment to change its behavior.[174][175][176][177]\\n\\nEssays on Automatics\\n\\nLeonardo Torres Quevedo\\n\\n\\n1921\\n\\nFirst fictional automatons called \"robots\" appear in the play R.U.R.\\n\\nRossum\\'s Universal Robots\\n\\nKarel Čapek\\n\\n\\n1930s\\n\\nHumanoid robot exhibited at the 1939 and 1940 World\\'s Fairs\\n\\nElektro\\n\\nWestinghouse Electric Corporation\\n\\n\\n1946\\n\\nFirst general-purpose digital computer\\n\\nWhirlwind\\n\\nMultiple people\\n\\n\\n1948\\n\\nSimple robots exhibiting biological behaviors[178]\\n\\nElsie and Elmer\\n\\nWilliam Grey Walter\\n\\n\\n1948\\n\\nFormulation of principles of cybernetics\\n\\ncybernetics\\n\\nNorbert Wiener\\n\\n\\n1956\\n\\nFirst commercial robot, from the Unimation company founded by George Devol and Joseph Engelberger, based on Devol\\'s patents[179]\\n\\nUnimate\\n\\nGeorge Devol\\n\\n\\n1961\\n\\nFirst installed industrial robot.  The first digitally operated and programmable robot, Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them.\\n\\nUnimate\\n\\nGeorge Devol\\n\\n\\n1967 to 1972\\n\\nFirst full-scale humanoid intelligent robot,[180][181] and first android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with its hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes, and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.[182][183][184]\\n\\nWABOT-1\\n\\nWaseda University\\n\\n\\n1973\\n\\nFirst industrial robot with six electromechanically driven axes[185][186]\\n\\nFamulus\\n\\nKUKA Robot Group\\n\\n\\n1974\\n\\nThe world\\'s first microcomputer controlled electric industrial robot, IRB 6 from ASEA, was delivered to a small mechanical engineering company in southern Sweden. The design of this robot had been patented in 1972.\\n\\nIRB 6\\n\\nABB Robot Group\\n\\n\\n1975\\n\\nProgrammable universal manipulation arm, a Unimation product\\n\\nPUMA\\n\\nVictor Scheinman\\n\\n\\n1978\\n\\nThe first object-level robot programming language, RAPT, allowing robots to handle variations in object position, shape, and sensor noise.[187]\\n\\nFreddy I and II\\n\\nPatricia Ambler and Robin Popplestone\\n\\n\\n1983\\n\\nFirst multitasking, the parallel programming language used for robot control. It was the Event Driven Language (EDL) on the IBM/Series/1 process computer, with the implementation of both inter-process communication (WAIT/POST) and mutual exclusion (ENQ/DEQ) mechanisms for robot control.[188]\\n\\nADRIEL I\\n\\nStevo Bozinovski and Mihail Sestakov\\n\\n\\nSee also[edit]\\n\\nArtificial intelligence\\nAutonomous robot\\nCloud robotics\\nCognitive robotics\\nEvolutionary robotics\\nFog robotics\\nGlossary of robotics\\nIndex of robotics articles\\nMechatronics\\nMulti-agent system\\nOutline of robotics\\nQuantum robotics\\nRoboethics\\nRobot rights\\nRobotic art\\nRobotic governance\\nSelf-reconfiguring modular robot\\nSoft robotics\\nTelerobotics\\n\\nNotes[edit]\\n\\n\\n^ One database, developed by the United States Department of Energy, contains information on almost 500 existing robotic technologies.[10]\\n\\n\\nReferences[edit]\\n\\n\\n^ \"German National Library\". International classification system of the German National Library (GND). Archived from the original on 2020-08-19.\\n\\n^ \"Origami-Inspired Robots Can Sense, Analyze and Act in Challenging Environments\". UCLA. Retrieved 2023-04-10.\\n\\n^ Raj, Aditi (26 August 2024). \"AI & Robotics: The Role of AI in Robots\". Retrieved 2024-08-29.\\n\\n^ Hunt, V. Daniel (1985). \"Smart Robots\". Smart Robots: A Handbook of Intelligent Robotic Systems. Chapman and Hall. p.\\xa0141. ISBN\\xa0978-1-4613-2533-8. Archived from the original on 2023-03-15. Retrieved 2018-12-04.\\n\\n^ \"Robot density rises globally\". Robotic Industries Association. 8 February 2018. Archived from the original on 2020-11-23. Retrieved 2018-12-03.\\n\\n^ Pinto, Jim (1 October 2003). \"Fully automated factories approach reality\". Automation World. Archived from the original on 2011-10-01. Retrieved 2018-12-03.\\n\\n^ Eyre, Michael (12 September 2014). \"\\'Boris\\' the robot can load up dishwasher\". BBC News. Archived from the original on 2020-12-21. Retrieved 2018-12-03.\\n\\n^ Corner, Stuart (23 November 2017). \"AI-driven robot makes \\'perfect\\' flatbread\". iothub.com.au. Archived from the original on 2020-11-24. Retrieved 2018-12-03.\\n\\n^ Pollock, Emily (7 June 2018). \"Construction Robotics Industry Set to Double by 2023\". engineering.com. Archived from the original on 2020-08-07. Retrieved 2018-12-03.\\n\\n^ \"Technology Advanced Search\". D&D Knowledge Management Information Tool. Archived from the original on 2020-08-06.\\n\\n^ Arámbula Cosío, F.; Hibberd, R. D.; Davies, B. L. (July 1997). \"Electromagnetic compatibility aspects of active robotic systems for surgery: the robotic prostatectomy experience\". Medical and Biological Engineering and Computing. 35 (4): 436–440. doi:10.1007/BF02534105. ISSN\\xa01741-0444. PMID\\xa09327627. S2CID\\xa021479700.\\n\\n^ Grift, Tony E. (2004). \"Agricultural Robotics\". University of Illinois at Urbana–Champaign. Archived from the original on 2007-05-04. Retrieved 2018-12-03.\\n\\n^ Thomas, Jim (1 November 2017). \"How corporate giants are automating the farm\". New Internationalist. Archived from the original on 2021-01-10. Retrieved 2018-12-03.\\n\\n^ Kolodny, Lora (4 July 2017). \"Robots are coming to a burger joint near you\". CNBC. Archived from the original on 2020-12-05. Retrieved 2018-12-03.\\n\\n^ Scott Kirsner (27 January 2023). \"Robots in the kitchen? Local engineers are making it a reality\". The Boston Globe.\\n\\n^ Dowling, Kevin. \"Power Sources for Small Robots\" (PDF). Carnegie Mellon University. Archived (PDF) from the original on 2020-11-25. Retrieved 2012-05-11.\\n\\n^ Roozing, Wesley; Li, Zhibin; Tsagarakis, Nikos; Caldwell, Darwin (2016). \"Design Optimisation and Control of Compliant Actuation Arrangements in Articulated Robots for Improved Energy Efficiency\". IEEE Robotics and Automation Letters. 1 (2): 1110–1117. doi:10.1109/LRA.2016.2521926. S2CID\\xa01940410.\\n\\n^ Pratt, G.A.; Williamson, M.M. (1995). \"Series elastic actuators\". Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human-Robot Interaction and Cooperative Robots. Vol.\\xa01. pp.\\xa0399–406. doi:10.1109/IROS.1995.525827. hdl:1721.1/36966. ISBN\\xa00-8186-7108-4. S2CID\\xa017120394.\\n\\n^ Furnémont, Raphaël; Mathijssen, Glenn; Verstraten, Tom; Lefeber, Dirk; Vanderborght, Bram (27 January 2016). \"Bi-directional series-parallel elastic actuator and overlap of the actuation layers\" (PDF). Bioinspiration & Biomimetics. 11 (1): 016005. Bibcode:2016BiBi...11a6005F. doi:10.1088/1748-3190/11/1/016005. PMID\\xa026813145. S2CID\\xa037031990. Archived (PDF) from the original on 2022-10-01. Retrieved 2023-03-15.\\n\\n^ Pratt, Jerry E.; Krupp, Benjamin T. (2004). \"Series Elastic Actuators for legged robots\". In Gerhart, Grant R; Shoemaker, Chuck M; Gage, Douglas W (eds.). Unmanned Ground Vehicle Technology VI. Vol.\\xa05422. pp.\\xa0135–144. doi:10.1117/12.548000. S2CID\\xa016586246.\\n\\n^ Li, Zhibin; Tsagarakis, Nikos; Caldwell, Darwin (2013). \"Walking Pattern Generation for a Humanoid Robot with Compliant Joints\". Autonomous Robots. 35 (1): 1–14. doi:10.1007/s10514-013-9330-7. S2CID\\xa0624563.\\n\\n^ Colgate, J. Edward (1988). The control of dynamically interacting systems (Thesis). hdl:1721.1/14380.\\n\\n^ Calanca, Andrea; Muradore, Riccardo; Fiorini, Paolo (November 2017). \"Impedance control of series elastic actuators: Passivity and acceleration-based control\". Mechatronics. 47: 37–48. doi:10.1016/j.mechatronics.2017.08.010.\\n\\n^ Tosun, Fatih Emre; Patoglu, Volkan (June 2020). \"Necessary and Sufficient Conditions for the Passivity of Impedance Rendering With Velocity-Sourced Series Elastic Actuation\". IEEE Transactions on Robotics. 36 (3): 757–772. doi:10.1109/TRO.2019.2962332. S2CID\\xa0212907787.\\n\\n^ www.imagesco.com, Images SI Inc -. \"Air Muscle actuators, going further, page 6\". Archived from the original on 2020-11-14. Retrieved 2010-05-24.\\n\\n^ \"Air Muscles\". Shadow Robot. Archived from the original on 2007-09-27.\\n\\n^ Tondu, Bertrand (2012). \"Modelling of the McKibben artificial muscle: A review\". Journal of Intelligent Material Systems and Structures. 23 (3): 225–253. doi:10.1177/1045389X11435435. S2CID\\xa0136854390.\\n\\n^ \"TALKING ELECTRONICS Nitinol Page-1\". Talkingelectronics.com. Archived from the original on 2020-01-18. Retrieved 2010-11-27.\\n\\n^ \"lf205, Hardware: Building a Linux-controlled walking robot\". Ibiblio.org. 1 November 2001. Archived from the original on 2016-03-03. Retrieved 2010-11-27.\\n\\n^ \"WW-EAP and Artificial Muscles\". Eap.jpl.nasa.gov. Archived from the original on 2017-01-20. Retrieved 2010-11-27.\\n\\n^ \"Empa – a117-2-eap\". Empa.ch. Archived from the original on 2015-09-24. Retrieved 2010-11-27.\\n\\n^ \"Electroactive Polymers (EAP) as Artificial Muscles (EPAM) for Robot Applications\". Hizook. Archived from the original on 2020-08-06. Retrieved 2010-11-27.\\n\\n^ \"Piezo LEGS – -09-26\". Archived from the original on 2008-01-30. Retrieved 2007-10-28.\\n\\n^ \"Squiggle Motors: Overview\". Archived from the original on 2007-10-07. Retrieved 2007-10-08.\\n\\n^ Nishibori; et\\xa0al. (2003). \"Robot Hand with Fingers Using Vibration-Type Ultrasonic Motors (Driving Characteristics)\". Journal of Robotics and Mechatronics. 15 (6): 588–595. doi:10.20965/jrm.2003.p0588.\\n\\n^ Otake, Mihoko; Kagami, Yoshiharu; Ishikawa, Kohei; Inaba, Masayuki; Inoue, Hirochika (6 April 2001). Wilson, Alan R.; Asanuma, Hiroshi (eds.). \"Shape design of gel robots made of electroactive polymer gel\". Smart Materials. 4234: 194–202. Bibcode:2001SPIE.4234..194O. doi:10.1117/12.424407. S2CID\\xa030357330.\\n\\n^ Madden, John D. (16 November 2007). \"Mobile Robots: Motor Challenges and Materials Solutions\". Science. 318 (5853): 1094–1097. Bibcode:2007Sci...318.1094M. CiteSeerX\\xa010.1.1.395.4635. doi:10.1126/science.1146351. PMID\\xa018006737. S2CID\\xa052827127.\\n\\n^ \"Syntouch LLC: BioTac(R) Biomimetic Tactile Sensor Array\". Archived from the original on 2009-10-03. Retrieved 2009-08-10.\\n\\n^ Wettels, Nicholas; Santos, Veronica J.; Johansson, Roland S.; Loeb, Gerald E. (January 2008). \"Biomimetic Tactile Sensor Array\". Advanced Robotics. 22 (8): 829–849. doi:10.1163/156855308X314533. S2CID\\xa04594917.\\n\\n^ \"What is The SmartHand?\". SmartHand Project. Archived from the original on 2015-03-03. Retrieved 2011-02-04.\\n\\n^ a b Arreguin, Juan (2008). Automation and Robotics. Vienna, Austria: I-Tech and Publishing.\\n\\n^ \"Annotated Mythbusters: Episode 78: Ninja Myths – Walking on Water, Catching a Sword, Catching an Arrow\". Archived from the original on 2020-11-12. Retrieved 2010-02-13. (Discovery Channel\\'s Mythbusters making mechanical gripper from the chain and metal wire)\\n\\n^ \"Robonaut hand\". Archived from the original on 2020-02-22. Retrieved 2011-11-21.\\n\\n^ \"Delft hand\". TU Delft. Archived from the original on 2012-02-03. Retrieved 2011-11-21.\\n\\n^ M&C. \"TU Delft ontwikkelt goedkope, voorzichtige robothand\". Archived from the original on 2017-03-13. Retrieved 2011-11-21.\\n\\n^ \"astrictive definition – English definition dictionary – Reverso\". Archived from the original on 2020-04-30. Retrieved 2008-01-06.\\n\\n^ Tijsma, H.A.; Liefhebber, F.; Herder, J.L. (2005). \"Evaluation of New User Interface Features for the MANUS Robot Arm\". 9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005. pp.\\xa0258–263. doi:10.1109/ICORR.2005.1501097. ISBN\\xa00-7803-9003-2. S2CID\\xa036445389.\\n\\n^ Allcock, Andrew (2006). \"Anthropomorphic hand is almost human\". Machinery. Archived from the original on 2007-09-28. Retrieved 2007-10-17.\\n\\n^ \"Welcome\". Archived (PDF) from the original on 2013-05-10. Retrieved 2007-10-28.\\n\\n^ a b c Corke, Peter (2017). Robotics, Vision and Control. Springer Tracts in Advanced Robotics. Vol.\\xa0118. doi:10.1007/978-3-319-54413-7. ISBN\\xa0978-3-319-54412-0. ISSN\\xa01610-7438. Archived from the original on 2022-10-20. Retrieved 2023-03-15.\\n\\n^ a b c Lee, K. S. Fu, Ralph Gonzalez, C S. G. (1987). Robotics: Control Sensing. Vis. McGraw-Hill. ISBN\\xa0978-0-07-026510-3. Archived from the original on 2023-03-15. Retrieved 2023-03-15.{{cite book}}:  CS1 maint: multiple names: authors list (link)\\n\\n^ a b c d Short, Michael; Burn, Kevin (1 April 2011). \"A generic controller architecture for intelligent robotic systems\". Robotics and Computer-Integrated Manufacturing. 27 (2): 292–305. doi:10.1016/j.rcim.2010.07.013. ISSN\\xa00736-5845.\\n\\n^ Ray, Partha Pratim (2016). \"Internet of Robotic Things: Concept, Technologies, and Challenges\". IEEE Access. 4: 9489–9500. Bibcode:2016IEEEA...4.9489R. doi:10.1109/ACCESS.2017.2647747. ISSN\\xa02169-3536. S2CID\\xa09273802.\\n\\n^ a b Burn, K.; Short, M.; Bicker, R. (July 2003). \"Adaptive and Nonlinear Fuzzy Force Control Techniques Applied to Robots Operating in Uncertain Environments\". Journal of Robotic Systems. 20 (7): 391–400. doi:10.1002/rob.10093. ISSN\\xa00741-2223. Archived from the original on 2022-11-26. Retrieved 2023-03-15.\\n\\n^ Burn, Kevin; Home, Geoffrey (1 May 2008). \"Environment classification using Kohonen self-organizing maps\". Expert Systems. 25 (2): 98–114. doi:10.1111/j.1468-0394.2008.00441.x. ISSN\\xa00266-4720. S2CID\\xa033369232.\\n\\n^ Mason, Matthew T. (2001). Mechanics of Robotic Manipulation. doi:10.7551/mitpress/4527.001.0001. ISBN\\xa09780262256629. S2CID\\xa05260407.\\n\\n^ \"What is a robotic end-effector?\". ATI Industrial Automation. 2007. Archived from the original on 2020-12-17. Retrieved 2007-10-16.\\n\\n^ Crane, Carl D.; Joseph Duffy (1998). Kinematic Analysis of Robot Manipulators. Cambridge University Press. ISBN\\xa0978-0-521-57063-3. Archived from the original on 2020-04-02. Retrieved 2007-10-16.\\n\\n^ G.J. Monkman, S. Hesse, R. Steinmann & H. Schunk (2007). Robot Grippers. Berlin: Wiley\\n\\n^ \"T.O.B.B\". Mtoussaint.de. Archived from the original on 2020-07-08. Retrieved 2010-11-27.\\n\\n^ \"nBot, a two wheel balancing robot\". Geology.heroy.smu.edu. Archived from the original on 2021-01-26. Retrieved 2010-11-27.\\n\\n^ \"ROBONAUT Activity Report\". NASA. 2004. Archived from the original on 2007-08-20. Retrieved 2007-10-20.\\n\\n^ Guizzo, Erico (29 April 2010). \"A Robot That Balances on a Ball\". IEEE Spectrum. Archived from the original on 2023-02-10. Retrieved 2023-03-15.\\n\\n^ \"Carnegie Mellon Researchers Develop New Type of Mobile Robot That Balances and Moves on a Ball Instead of Legs or Wheels\" (Press release). Carnegie Mellon. 9 August 2006. Archived from the original on 2007-06-09. Retrieved 2007-10-20.\\n\\n^ \"Spherical Robot Can Climb Over Obstacles\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27.\\n\\n^ \"Rotundus\". Rotundus.se. Archived from the original on 2011-08-26. Retrieved 2010-11-27.\\n\\n^ \"OrbSwarm Gets A Brain\". BotJunkie. 11 July 2007. Archived from the original on 2012-05-16. Retrieved 2010-11-27.\\n\\n^ \"Rolling Orbital Bluetooth Operated Thing\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27.\\n\\n^ \"Swarm\". Orbswarm.com. Archived from the original on 2021-01-26. Retrieved 2010-11-27.\\n\\n^ \"The Ball Bot\\xa0: Johnnytronic@Sun\". Blogs.sun.com. Archived from the original on 2011-08-24. Retrieved 2010-11-27.\\n\\n^ \"Senior Design Projects | College of Engineering & Applied Science| University of Colorado at Boulder\". Engineering.colorado.edu. 30 April 2008. Archived from the original on 2011-07-23. Retrieved 2010-11-27.\\n\\n^ \"JPL Robotics: System: Commercial Rovers\". Archived from the original on 2006-06-15.\\n\\n^ \"AMBER Lab\". Archived from the original on 2020-11-25. Retrieved 2012-01-23.\\n\\n^ \"Micromagic Systems Robotics Lab\". Archived from the original on 2017-06-01. Retrieved 2009-04-29.\\n\\n^ \"AMRU-5 hexapod robot\" (PDF). Archived (PDF) from the original on 2016-08-17. Retrieved 2009-04-29.\\n\\n^ \"Achieving Stable Walking\". Honda Worldwide. Archived from the original on 2011-11-08. Retrieved 2007-10-22.\\n\\n^ \"Funny Walk\". Pooter Geek. 28 December 2004. Archived from the original on 2011-09-28. Retrieved 2007-10-22.\\n\\n^ \"ASIMO\\'s Pimp Shuffle\". Popular Science. 9 January 2007. Archived from the original on 2011-07-24. Retrieved 2007-10-22.\\n\\n^ \"Robot Shows Prime Minister How to Loosen Up > > A drunk robot?\". The Temple of VTEC – Honda and Acura Enthusiasts Online Forums. 25 August 2003. Archived from the original on 2020-04-30.\\n\\n^ \"3D One-Leg Hopper (1983–1984)\". MIT Leg Laboratory. Archived from the original on 2018-07-25. Retrieved 2007-10-22.\\n\\n^ \"3D Biped (1989–1995)\". MIT Leg Laboratory. Archived from the original on 2011-09-26. Retrieved 2007-10-28.\\n\\n^ \"Quadruped (1984–1987)\". MIT Leg Laboratory. Archived from the original on 2011-08-23. Retrieved 2007-10-28.\\n\\n^ \"MIT Leg Lab Robots- Main\". Archived from the original on 2020-08-07. Retrieved 2007-10-28.\\n\\n^ \"About the Robots\". Anybots. Archived from the original on 2007-09-09. Retrieved 2007-10-23.\\n\\n^ \"Anything, Anytime, Anywhere\". Anybots. Archived from the original on 2007-10-27. Retrieved 2007-10-23.\\n\\n^ \"Dexter Jumps video\". YouTube. 1 March 2007. Archived from the original on 2021-10-30. Retrieved 2007-10-23.\\n\\n^ Collins, Steve; Ruina, Andy; Tedrake, Russ; Wisse, Martijn (18 February 2005). \"Efficient Bipedal Robots Based on Passive-Dynamic Walkers\". Science. 307 (5712): 1082–1085. Bibcode:2005Sci...307.1082C. doi:10.1126/science.1107799. PMID\\xa015718465. S2CID\\xa01315227.\\n\\n^ Collins, S.H.; Ruina, A. (2005). \"A Bipedal Walking Robot with Efficient and Human-Like Gait\". Proceedings of the 2005 IEEE International Conference on Robotics and Automation. pp.\\xa01983–1988. doi:10.1109/ROBOT.2005.1570404. ISBN\\xa00-7803-8914-X. S2CID\\xa015145353.\\n\\n^ \"Testing the Limits\" (PDF). Boeing. p.\\xa029. Archived (PDF) from the original on 2018-12-15. Retrieved 2008-04-09.\\n\\n^ Zhang, Jun; Zhao, Ning; Qu, Feiyang (15 November 2022). \"Bio-inspired flapping wing robots with foldable or deformable wings: a review\". Bioinspiration & Biomimetics. 18 (1): 011002. doi:10.1088/1748-3190/ac9ef5. ISSN\\xa01748-3182. PMID\\xa036317380. S2CID\\xa0253246037.\\n\\n^ a b c Shin, Won Dong; Park, Jaejun; Park, Hae-Won (1 September 2019). \"Development and experiments of a bio-inspired robot with multi-mode in aerial and terrestrial locomotion\". Bioinspiration & Biomimetics. 14 (5): 056009. Bibcode:2019BiBi...14e6009S. doi:10.1088/1748-3190/ab2ab7. ISSN\\xa01748-3182. PMID\\xa031212268. S2CID\\xa0195066183.\\n\\n^ Ramezani, Alireza; Shi, Xichen; Chung, Soon-Jo; Hutchinson, Seth (May 2016). \"Bat Bot (B2), a biologically inspired flying machine\". 2016 IEEE International Conference on Robotics and Automation (ICRA). Stockholm, Sweden: IEEE. pp.\\xa03219–3226. doi:10.1109/ICRA.2016.7487491. ISBN\\xa0978-1-4673-8026-3. S2CID\\xa08581750.\\n\\n^ a b Daler, Ludovic; Mintchev, Stefano; Stefanini, Cesare; Floreano, Dario (19 January 2015). \"A bioinspired multi-modal flying and walking robot\". Bioinspiration & Biomimetics. 10 (1): 016005. Bibcode:2015BiBi...10a6005D. doi:10.1088/1748-3190/10/1/016005. ISSN\\xa01748-3190. PMID\\xa025599118. S2CID\\xa011132948.\\n\\n^ a b Kilian, Lukas; Shahid, Farzeen; Zhao, Jing-Shan; Nayeri, Christian Navid (1 July 2022). \"Bioinspired morphing wings: mechanical design and wind tunnel experiments\". Bioinspiration & Biomimetics. 17 (4): 046019. Bibcode:2022BiBi...17d6019K. doi:10.1088/1748-3190/ac72e1. ISSN\\xa01748-3182. PMID\\xa035609562. S2CID\\xa0249045806.\\n\\n^ Savastano, E.; Perez-Sanchez, V.; Arrue, B.C.; Ollero, A. (July 2022). \"High-Performance Morphing Wing for Large-Scale Bio-Inspired Unmanned Aerial Vehicles\". IEEE Robotics and Automation Letters. 7 (3): 8076–8083. doi:10.1109/LRA.2022.3185389. ISSN\\xa02377-3766. S2CID\\xa0250008824.\\n\\n^ Grant, Daniel T.; Abdulrahim, Mujahid; Lind, Rick (June 2010). \"Flight Dynamics of a Morphing Aircraft Utilizing Independent Multiple-Joint Wing Sweep\". International Journal of Micro Air Vehicles. 2 (2): 91–106. doi:10.1260/1756-8293.2.2.91. ISSN\\xa01756-8293. S2CID\\xa0110577545.\\n\\n^ Phan, Hoang Vu; Park, Hoon Cheol (4 December 2020). \"Mechanisms of collision recovery in flying beetles and flapping-wing robots\". Science. 370 (6521): 1214–1219. Bibcode:2020Sci...370.1214P. doi:10.1126/science.abd3285. ISSN\\xa00036-8075. PMID\\xa033273101. S2CID\\xa0227257247.\\n\\n^ Hu, Zheng; McCauley, Raymond; Schaeffer, Steve; Deng, Xinyan (May 2009). \"Aerodynamics of dragonfly flight and robotic design\". 2009 IEEE International Conference on Robotics and Automation. pp.\\xa03061–3066. doi:10.1109/ROBOT.2009.5152760. ISBN\\xa0978-1-4244-2788-8. S2CID\\xa012291429.\\n\\n^ Balta, Miquel; Deb, Dipan; Taha, Haithem E (26 October 2021). \"Flow visualization and force measurement of the clapping effect in bio-inspired flying robots\". Bioinspiration & Biomimetics. 16 (6): 066020. Bibcode:2021BiBi...16f6020B. doi:10.1088/1748-3190/ac2b00. ISSN\\xa01748-3182. PMID\\xa034584023. S2CID\\xa0238217893.\\n\\n^ Miller, Gavin. \"Introduction\". snakerobots.com. Archived from the original on 2011-08-17. Retrieved 2007-10-22.\\n\\n^ \"ACM-R5\". Archived from the original on 2011-10-11.\\n\\n^ \"Swimming snake robot (commentary in Japanese)\". Archived from the original on 2012-02-08. Retrieved 2007-10-28.\\n\\n^ \"Commercialized Quadruped Walking Vehicle \"TITAN VII\"\". Hirose Fukushima Robotics Lab. Archived from the original on 2007-11-06. Retrieved 2007-10-23.\\n\\n^ Pachal, Peter (23 January 2007). \"Plen, the robot that skates across your desk\". SCI FI Tech. Archived from the original on 2007-10-11.\\n\\n^ Capuchin on YouTube\\n\\n^ Wallbot on YouTube\\n\\n^ Stanford University: Stickybot on YouTube\\n\\n^ Sfakiotakis, M.; Lane, D.M.; Davies, J.B.C. (April 1999). \"Review of fish swimming modes for aquatic locomotion\". IEEE Journal of Oceanic Engineering. 24 (2): 237–252. Bibcode:1999IJOE...24..237S. CiteSeerX\\xa010.1.1.459.8614. doi:10.1109/48.757275. S2CID\\xa017226211.\\n\\n^ Richard Mason. \"What is the market for robot fish?\". Archived from the original on 2009-07-04.\\n\\n^ \"Robotic fish powered by Gumstix PC and PIC\". Human Centred Robotics Group at Essex University. Archived from the original on 2011-08-14. Retrieved 2007-10-25.\\n\\n^ Witoon Juwarahawong. \"Fish Robot\". Institute of Field Robotics. Archived from the original on 2007-11-04. Retrieved 2007-10-25.\\n\\n^ \"Festo - AquaPenguin\". 17 April 2009 – via YouTube.\\n\\n^ \"High-Speed Robotic Fish\". iSplash-Robotics. Archived from the original on 2020-03-11. Retrieved 2017-01-07.\\n\\n^ \"iSplash-II: Realizing Fast Carangiform Swimming to Outperform a Real Fish\" (PDF). Robotics Group at Essex University. Archived from the original (PDF) on 2015-09-30. Retrieved 2015-09-29.\\n\\n^ \"iSplash-I: High Performance Swimming Motion of a Carangiform Robotic Fish with Full-Body Coordination\" (PDF). Robotics Group at Essex University. Archived from the original (PDF) on 2015-09-30. Retrieved 2015-09-29.\\n\\n^ Jaulin, Luc; Le Bars, Fabrice (February 2013). \"An Interval Approach for Stability Analysis: Application to Sailboat Robotics\". IEEE Transactions on Robotics. 29 (1): 282–287. CiteSeerX\\xa010.1.1.711.7180. doi:10.1109/TRO.2012.2217794. S2CID\\xa04977937.\\n\\n^ \"A Ping-Pong-Playing Terminator\". Popular Science. Archived from the original on 2021-01-22. Retrieved 2010-12-19.\\n\\n^ \"Synthiam Exosphere combines AI, human operators to train robots\". The Robot Report. Archived from the original on 2020-10-06. Retrieved 2020-04-29.\\n\\n^ a b Kagan, Eugene; Ben-Gal, Irad (2015). Search and foraging:individual motion and swarm dynamics. Chapman and Hall/CRC. ISBN\\xa09781482242102. Archived from the original on 2023-03-15. Retrieved 2020-08-26.\\n\\n^ Banks, Jaime (2020). \"Optimus Primed: Media Cultivation of Robot Mental Models and Social Judgments\". Frontiers in Robotics and AI. 7: 62. doi:10.3389/frobt.2020.00062. PMC\\xa07805817. PMID\\xa033501230.\\n\\n^ a b Wullenkord, Ricarda; Fraune, Marlena R.; Eyssel, Friederike; Sabanovic, Selma (2016). \"Getting in Touch: How imagined, actual, and physical contact affect evaluations of robots\". 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). pp.\\xa0980–985. doi:10.1109/ROMAN.2016.7745228. ISBN\\xa0978-1-5090-3929-6. S2CID\\xa06305599.\\n\\n^ Norberto Pires, J. (December 2005). \"Robot-by-voice: experiments on commanding an industrial robot using the human voice\". Industrial Robot. 32 (6): 505–511. doi:10.1108/01439910510629244.\\n\\n^ \"Survey of the State of the Art in Human Language Technology: 1.2: Speech Recognition\". Archived from the original on 2007-11-11.\\n\\n^ Fournier, Randolph Scott; Schmidt, B. June (1995). \"Voice input technology: Learning style and attitude toward its use\". Delta Pi Epsilon Journal. 37 (1): 1–12. ProQuest\\xa01297783046.\\n\\n^ \"History of Speech & Voice Recognition and Transcription Software\". Dragon Naturally Speaking. Archived from the original on 2015-08-13. Retrieved 2007-10-27.\\n\\n^ Cheng Lin, Kuan; Huang, Tien-Chi; Hung, Jason C.; Yen, Neil Y.; Ju Chen, Szu (7 June 2013). \"Facial emotion recognition towards affective computing-based learning\". Library Hi Tech. 31 (2): 294–307. doi:10.1108/07378831311329068.\\n\\n^ Walters, M. L.; Syrdal, D. S.; Koay, K. L.; Dautenhahn, K.; Te Boekhorst, R. (2008). \"Human approach distances to a mechanical-looking robot with different robot voice styles\". RO-MAN 2008 - the 17th IEEE International Symposium on Robot and Human Interactive Communication. pp.\\xa0707–712. doi:10.1109/ROMAN.2008.4600750. ISBN\\xa0978-1-4244-2212-8. S2CID\\xa08653718.\\n\\n^ Pauletto, Sandra; Bowles, Tristan (2010). \"Designing the emotional content of a robotic speech signal\". Proceedings of the 5th Audio Mostly Conference on a Conference on Interaction with Sound - AM \\'10. pp.\\xa01–8. doi:10.1145/1859799.1859804. ISBN\\xa0978-1-4503-0046-9. S2CID\\xa030423778.\\n\\n^ Bowles, Tristan; Pauletto, Sandra (2010). Emotions in the Voice: Humanising a Robotic Voice (PDF). Proceedings of the 7th Sound and Music Computing Conference. Barcelona. Archived (PDF) from the original on 2023-02-10. Retrieved 2023-03-15.\\n\\n^ \"World of 2-XL: Leachim\". www.2xlrobot.com. Archived from the original on 2020-07-05. Retrieved 2019-05-28.\\n\\n^ \"The Boston Globe from Boston, Massachusetts on June 23, 1974 · 132\". Newspapers.com. 23 June 1974. Archived from the original on 2020-01-10. Retrieved 2019-05-28.\\n\\n^ a b \"cyberneticzoo.com - Page 135 of 194 - a history of cybernetic animals and early robots\". cyberneticzoo.com. Archived from the original on 2020-08-06. Retrieved 2019-05-28.\\n\\n^ \"Frubber facial expressions\". Archived from the original on 2009-02-07.\\n\\n^ \"Best Inventions of 2008 – TIME\". Time. 29 October 2008. Archived from the original on 2008-11-02 – via www.time.com.\\n\\n^ \"Kismet: Robot at MIT\\'s AI Lab Interacts With Humans\". Sam Ogden. Archived from the original on 2007-10-12. Retrieved 2007-10-28.\\n\\n^ Waldherr, Stefan; Romero, Roseli; Thrun, Sebastian (1 September 2000). \"A Gesture Based Interface for Human-Robot Interaction\". Autonomous Robots. 9 (2): 151–173. doi:10.1023/A:1008918401478. S2CID\\xa01980239.\\n\\n^ Li, Ling Hua; Du, Ji Fang (December 2012). \"Visual Based Hand Gesture Recognition Systems\". Applied Mechanics and Materials. 263–266: 2422–2425. Bibcode:2012AMM...263.2422L. doi:10.4028/www.scientific.net/AMM.263-266.2422. S2CID\\xa062744240.\\n\\n^ \"Armenian Robin the Robot to comfort kids at U.S. clinics starting July\". Public Radio of Armenia. Archived from the original on 2021-05-13. Retrieved 2021-05-13.\\n\\n^ Park, S.; Sharlin, Ehud; Kitamura, Y.; Lau, E. (29 April 2005). Synthetic Personality in Robots and its Effect on Human-Robot Relationship (Report). doi:10.11575/PRISM/31041. hdl:1880/45619.\\n\\n^ \"Robot Receptionist Dishes Directions and Attitude\". NPR.org. Archived from the original on 2020-12-01. Retrieved 2018-04-05.\\n\\n^ \"New Scientist: A good robot has personality but not looks\" (PDF). Archived from the original (PDF) on 2006-09-29.\\n\\n^ \"Playtime with Pleo, your robotic dinosaur friend\". 25 September 2008. Archived from the original on 2019-01-20. Retrieved 2014-12-14.\\n\\n^ NOVA conversation with Professor Moravec, October 1997. NOVA Online Archived 2017-08-02 at the Wayback Machine\\n\\n^ Agarwal, P.K. Elements of Physics XI. Rastogi Publications. p.\\xa02. ISBN\\xa0978-81-7133-911-2.\\n\\n^ Sandhana, Lakshmi (5 September 2002). \"A Theory of Evolution, for Robots\". Wired. Archived from the original on 2014-03-29. Retrieved 2007-10-28.\\n\\n^ \"Experimental Evolution In Robots Probes The Emergence Of Biological Communication\". Science Daily. 24 February 2007. Archived from the original on 2018-11-16. Retrieved 2007-10-28.\\n\\n^ Žlajpah, Leon (15 December 2008). \"Simulation in robotics\". Mathematics and Computers in Simulation. 79 (4): 879–897. doi:10.1016/j.matcom.2008.02.017.\\n\\n^ \"Evolution trains robot teams TRN 051904\". Technology Research News. Archived from the original on 2016-06-23. Retrieved 2009-01-22.\\n\\n^ Tandon, Prateek (2017). Quantum Robotics. Morgan & Claypool Publishers. ISBN\\xa0978-1627059138.\\n\\n^ Dragani, Rachelle (8 November 2018). \"Can a robot make you a \\'superworker\\'?\". Verizon Communications. Archived from the original on 2020-08-06. Retrieved 2018-12-03.\\n\\n^ \"Robotics\". American Elements. Retrieved 2023-04-10.\\n\\n^ \"Career: Robotics Engineer\". Princeton Review. 2012. Archived from the original on 2015-01-21. Retrieved 2012-01-27.\\n\\n^ Saad, Ashraf; Kroutil, Ryan (2012). Hands-on Learning of Programming Concepts Using Robotics for Middle and High School Students. Proceedings of the 50th Annual Southeast Regional Conference of the Association for Computing Machinery. ACM. pp.\\xa0361–362. doi:10.1145/2184512.2184605.\\n\\n^ Toy, Tommy (29 June 2011). \"Outlook for robotics and Automation for 2011 and beyond are excellent says expert\". PBT Consulting. Archived from the original on 2012-01-27. Retrieved 2012-01-27.\\n\\n^ Frey, Carl Benedikt; Osborne, Michael A. (January 2017). \"The future of employment: How susceptible are jobs to computerisation?\". Technological Forecasting and Social Change. 114: 254–280. CiteSeerX\\xa010.1.1.395.416. doi:10.1016/j.techfore.2016.08.019.\\n\\n^ McGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID\\xa0243172487. SSRN\\xa03044448.\\n\\n^ Hawking, Stephen (1 January 2016). \"This is the most dangerous time for our planet\". The Guardian. Archived from the original on 2021-01-31. Retrieved 2019-11-22.\\n\\n^ \"Robotics – Thematic Research\". GlobalData. Archived from the original on 2021-09-28. Retrieved 2021-09-22.\\n\\n^ \"Focal Points Seminar on review articles in the future of work – Safety and health at work – EU-OSHA\". osha.europa.eu. Archived from the original on 2020-01-25. Retrieved 2016-04-19.\\n\\n^ \"Robotics: Redefining crime prevention, public safety and security\". SourceSecurity.com. Archived from the original on 2017-10-09. Retrieved 2016-09-16.\\n\\n^ \"Draft Standard for Intelligent Assist Devices — Personnel Safety Requirements\" (PDF). Archived (PDF) from the original on 2020-11-25. Retrieved 2016-06-01.\\n\\n^ \"ISO/TS 15066:2016 – Robots and robotic devices – Collaborative robots\". 8 March 2016. Archived from the original on 2016-10-10. Retrieved 2016-06-01.\\n\\n^ Brogårdh, Torgny (January 2007). \"Present and future robot control development—An industrial perspective\". Annual Reviews in Control. 31 (1): 69–79. doi:10.1016/j.arcontrol.2007.01.002. ISSN\\xa01367-5788.\\n\\n^ Wang, Tian-Miao; Tao, Yong; Liu, Hui (17 April 2018). \"Current Researches and Future Development Trend of Intelligent Robot: A Review\". International Journal of Automation and Computing. 15 (5): 525–546. doi:10.1007/s11633-018-1115-1. ISSN\\xa01476-8186. S2CID\\xa0126037910. Archived from the original on 2023-03-15. Retrieved 2023-03-15.\\n\\n^ Needham, Joseph (1991). Science and Civilisation in China: Volume 2, History of Scientific Thought. Cambridge University Press. ISBN\\xa0978-0-521-05800-1.\\n\\n^ Fowler, Charles B. (October 1967). \"The Museum of Music: A History of Mechanical Instruments\". Music Educators Journal. 54 (2): 45–49. doi:10.2307/3391092. JSTOR\\xa03391092. S2CID\\xa0190524140.\\n\\n^ Rosheim, Mark E. (1994). Robot Evolution: The Development of Anthrobotics. Wiley-IEEE. pp.\\xa09–10. ISBN\\xa0978-0-471-02622-8.\\n\\n^ al-Jazari (Islamic artist) Archived 2008-05-07 at the Wayback Machine, Encyclopædia Britannica.\\n\\n^ A. P. Yuste. Electrical Engineering Hall of Fame. Early Developments of Wireless Remote Control: The Telekino of Torres-Quevedo,(pdf) vol. 96, No. 1, January 2008, Proceedings of the IEEE.\\n\\n^ H. R. Everett (2015). Unmanned Systems of World Wars I and II. MIT Press. pp.\\xa091–95. ISBN\\xa0978-0-262-02922-3.\\n\\n^ Randy Alfred, \"Nov. 7, 1905: Remote Control Wows Public\", Wired, 7 November 2011.\\n\\n^ Williams, Andrew (16 March 2017). History of Digital Games: Developments in Art, Design and Interaction. CRC Press. ISBN\\xa09781317503811.\\n\\n^ Randell, Brian (October 1982). \"From Analytical Engine to Electronic Digital Computer: The Contributions of Ludgate, Torres, and Bush\". IEEE Annals of the History of Computing. 4 (4): 327–341. doi:10.1109/MAHC.1982.10042. S2CID\\xa01737953.\\n\\n^ L. Torres Quevedo. Ensayos sobre Automática - Su definicion. Extension teórica de sus aplicaciones, Revista de la Academia de Ciencias Exacta, Revista 12, pp.391-418, 1914.\\n\\n^ Torres Quevedo, Leonardo. Automática: Complemento de la Teoría de las Máquinas, (pdf), pp. 575-583, Revista de Obras Públicas, 19 November 1914.\\n\\n^ L. Torres Quevedo. Essais sur l\\'Automatique - Sa définition. Etendue théorique de ses applications Archived 2023-02-10 at the Wayback Machine, Revue Génerale des Sciences Pures et Appliquées, vol.2, pp.601-611, 1915.\\n\\n^ B. Randell. Essays on Automatics, The Origins of Digital Computers, pp.89-107, 1982.\\n\\n^ PhD, Renato M.E. Sabbatini. \"Sabbatini, RME: An Imitation of Life: The First Robots\". Archived from the original on 2009-07-20. Retrieved 2023-03-15.\\n\\n^ Waurzyniak, Patrick (2006). \"Masters of Manufacturing: Joseph F. Engelberger\". Society of Manufacturing Engineers. 137 (1). Archived from the original on 2011-11-09.\\n\\n^ \"Humanoid History -WABOT-\". www.humanoid.waseda.ac.jp. Archived from the original on 2017-09-01. Retrieved 2017-05-06.\\n\\n^ Zeghloul, Saïd; Laribi, Med Amine; Gazeau, Jean-Pierre (21 September 2015). Robotics and Mechatronics: Proceedings of the 4th IFToMM International Symposium on Robotics and Mechatronics. Springer. ISBN\\xa09783319223681. Archived from the original on 2023-03-15. Retrieved 2017-09-10 – via Google Books.\\n\\n^ \"Historical Android Projects\". androidworld.com. Archived from the original on 2005-11-25. Retrieved 2017-05-06.\\n\\n^ Robots: From Science Fiction to Technological Revolution Archived 2023-03-15 at the Wayback Machine, page 130\\n\\n^ Duffy, Vincent G. (19 April 2016). Handbook of Digital Human Modeling: Research for Applied Ergonomics and Human Factors Engineering. CRC Press. ISBN\\xa09781420063523. Archived from the original on 2023-03-15. Retrieved 2017-09-10 – via Google Books.\\n\\n^ \"KUKA Industrial Robot FAMULUS\". Archived from the original on 2009-02-20. Retrieved 2008-01-10.\\n\\n^ \"History of Industrial Robots\" (PDF). Archived from the original (PDF) on 2012-12-24. Retrieved 2012-10-27.\\n\\n^ R. J. Popplestone; A. P. Ambler; I. Bellos (1978). \"RAPT: A language for describing assemblies\". Industrial Robot. 5 (3): 131–137. doi:10.1108/eb004501.\\n\\n^ Bozinovski, S. (1994). \"Parallel programming for mobile robot control: Agent-based approach\". 14th International Conference on Distributed Computing Systems. pp.\\xa0202–208. doi:10.1109/ICDCS.1994.302412. ISBN\\xa00-8186-5840-1. S2CID\\xa027855786.\\n\\n\\nFurther reading[edit]\\nR. Andrew Russell (1990). Robot Tactile Sensing. New York: Prentice Hall. ISBN\\xa0978-0-13-781592-0.\\nMcGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID\\xa0243172487. SSRN\\xa03044448.\\nAutor, David H. (1 August 2015). \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\". Journal of Economic Perspectives. 29 (3): 3–30. doi:10.1257/jep.29.3.3. hdl:1721.1/109476.\\nTooze, Adam (6 June 2019). \"Democracy and Its Discontents\". The New York Review of Books. Vol.\\xa066, no.\\xa010.\\nExternal links[edit]\\n\\n\\nRobotics  at Wikipedia\\'s sister projects\\n\\nDefinitions from WiktionaryMedia from CommonsTextbooks from WikibooksResources from Wikiversity\\n\\n‹The template Curlie is being considered for deletion.›\\xa0Robotics at Curlie\\nIEEE Robotics and Automation Society\\nInvestigation of social robots – Robots that mimic human behaviors and gestures.\\nWired\\'s guide to the \\'50 best robots ever\\', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).\\nvteRoboticsMain articles\\nOutline\\nGlossary\\nIndex\\nHistory\\nGeography\\nHall of Fame\\nEthics\\nLaws\\nCompetitions\\nAI competitions\\nTypes\\nAerobot\\nAnthropomorphic\\nHumanoid\\nAndroid\\nCyborg\\nGynoid\\nClaytronics\\nCompanion\\nAutomaton\\nAnimatronic\\nAudio-Animatronics\\nIndustrial\\nArticulated\\narm\\nDomestic\\nEducational\\nEntertainment\\nJuggling\\nMilitary\\nMedical\\nService\\nDisability\\nAgricultural\\nFood service\\nRetail\\nBEAM robotics\\nSoft robotics\\nClassifications\\nBiorobotics\\nCloud robotics\\nContinuum robot\\nUnmanned vehicle\\naerial\\nground\\nMobile robot\\nMicrobotics\\nNanorobotics\\nNecrobotics\\nRobotic spacecraft\\nSpace probe\\nSwarm\\nTelerobotics\\nUnderwater\\nremotely-operated\\nRobotic fish\\nLocomotion\\nTracks\\nWalking\\nHexapod\\nClimbing\\nElectric unicycle\\nRobotic fins\\nNavigation and mapping\\nMotion planning\\nSimultaneous localization and mapping\\nVisual odometry\\nVision-guided robot systems\\nResearch\\nEvolutionary\\nKits\\nSimulator\\nSuite\\nOpen-source\\nSoftware\\nAdaptable\\nDevelopmental\\nHuman–robot interaction\\nParadigms\\nPerceptual\\nSituated\\nUbiquitous\\nCompanies\\nAmazon Robotics\\nAnybots\\nBarrett Technology\\nBoston Dynamics\\nEnergid Technologies\\nFarmWise\\nFANUC\\nFigure AI\\nFoster-Miller\\nHarvest Automation\\nHoneybee Robotics\\nIntuitive Surgical\\nIRobot\\nKUKA\\nStarship Technologies\\nSymbotic\\nUniversal Robotics\\nWolf Robotics\\nYaskawa\\nRelated\\nCritique of work\\nPowered exoskeleton\\nWorkplace robotics safety\\nRobotic tech vest\\nTechnological unemployment\\nTerrainability\\nFictional robots\\n\\n Category\\n Outline\\n\\nvteEngineering\\nHistory\\nOutline\\nList of engineering branches\\nSpecialtiesandInterdisciplinarityCivil\\nArchitectural\\nCoastal\\nConstruction\\nEarthquake\\nEnvironmental\\nEcological\\nSanitary\\nGeological\\nGeotechnical\\nHydraulic\\nMining\\nMunicipal/Urban\\nOffshore\\nRiver\\nStructural\\nTransportation\\nTraffic\\nRailway\\nMechanical\\nAcoustic\\nAerospace\\nAutomotive\\nBiomechanical\\nEnergy\\nManufacturing\\nMarine\\nNaval architecture\\nRailway\\nSports\\nThermal\\nTribology\\nElectrical\\nBroadcast\\nComputer\\noutline\\nControl\\nElectromechanics\\nElectronics\\nMicrowaves\\nOptical\\nPower\\nRadio frequency\\nSignal processing\\nTelecommunications\\nChemical\\nBiochemical/Bioprocess\\nBiological\\nBioresource\\nGenetic\\nTissue\\nChemical reaction\\nElectrochemical\\nFood\\nMolecular\\nPaper\\nPetroleum\\nProcess\\nReaction\\nMaterials\\nBiomaterial\\nCeramics\\nCorrosion\\nMetallurgy\\nMolecular\\nNanotechnology\\nPolymers\\nSemiconductors\\nSurfaces\\nOther\\nAgricultural\\nAudio\\nAutomation\\nBiomedical\\nBioinformatics\\nClinical\\nHealth technology\\nPharmaceutical\\nRehabilitation\\nBuilding services\\nMEP\\nGeoengineering\\nDesign\\nEngineering drawing/graphics\\nEngineering management\\nEngineering mathematics\\nEngineering physics\\nExplosives\\nFacilities\\nFire\\nForensic\\nGeomatics\\nIndustrial\\nInformation\\nInstrumentation\\nand Control\\nLogistics\\nRobotics\\nMechatronics\\nMilitary\\nNuclear\\nOntology\\nPackaging\\nPrivacy\\nSafety\\nSurvey\\nSecurity\\nSoftware\\nSustainability\\nSystems\\nTextile\\nEngineering education\\nBachelor of Engineering\\nBachelor of Science\\nMaster\\'s degree\\nDoctorate\\nGraduate certificate\\nEngineer\\'s degree\\nLicensed engineer\\nRelated topics\\nEngineer\\nGlossaries\\nEngineering\\nA–L\\nM–Z\\nAerospace engineering\\nCivil engineering\\nElectrical and electronics engineering\\nMechanical engineering\\nStructural engineering\\n\\n Category\\n Commons\\n Wikiproject\\n Portal\\n\\nvteEmerging technologiesFieldsManufacturing\\n3D microfabrication\\n3D printing\\n3D publishing\\nClaytronics\\nMolecular assembler\\nSmart manufacturing\\nUtility fog\\nMaterials science\\nAerogel\\nAmorphous metal\\nArtificial muscle\\nConductive polymer\\nFemtotechnology\\nFullerene\\nGraphene\\nHigh-temperature superconductivity\\nHigh-temperature superfluidity\\nLinear acetylenic carbon\\nMetamaterials\\nMetamaterial cloaking\\nMetal foam\\nMulti-function structures\\nNanotechnology\\nCarbon nanotubes\\nMolecular nanotechnology\\nNanomaterials\\nPicotechnology\\nProgrammable matter\\nQuantum dots\\nSilicene\\nSynthetic diamond\\nRobotics\\nDomotics\\nNanorobotics\\nPowered exoskeleton\\nSelf-reconfiguring modular robot\\nSwarm robotics\\nUncrewed vehicle\\nTopics\\nAutomation\\nCollingridge dilemma\\nDifferential technological development\\nDisruptive innovation\\nEphemeralization\\nEthics\\nBioethics\\nCyberethics\\nNeuroethics\\nRobot ethics\\nExploratory engineering\\nProactionary principle\\nTechnological change\\nTechnological unemployment\\nTechnological convergence\\nTechnological evolution\\nTechnological paradigm\\nTechnology forecasting\\nAccelerating change\\nFuture-oriented technology analysis\\nHorizon scanning\\nMoore\\'s law\\nTechnological singularity\\nTechnology scouting\\nTechnology in science fiction\\nTechnology readiness level\\nTechnology roadmap\\nTranshumanism\\n\\n List\\n\\nvteGlossaries of science and engineering\\nAerospace engineering\\nAgriculture\\nArchaeology\\nArchitecture\\nArtificial intelligence\\nAstronomy\\nBiology\\nBotany\\nCalculus\\nCell biology\\nChemistry\\nCivil engineering\\nClinical research\\nComputer hardware\\nComputer science\\nDevelopmental and reproductive biology\\nEcology\\nEconomics\\nElectrical and electronics engineering\\nEngineering\\nA–L\\nM–Z\\nEntomology\\nEnvironmental science\\nGenetics and evolutionary biology\\nCellular and molecular biology\\n0–L\\nM–Z\\nGeography\\nA–M\\nN–Z\\nArabic toponyms\\nHebrew toponyms\\nWestern and South Asia\\nGeology\\nIchthyology\\nMachine vision\\nMathematics\\nMechanical engineering\\nMedicine\\nMeteorology\\nMycology\\nNanotechnology\\nOrnithology\\nPhysics\\nProbability and statistics\\nPsychiatry\\nQuantum computing\\nRobotics\\nScientific naming\\nStructural engineering\\nVirology\\n\\nAuthority control databases NationalGermanyUnited StatesFranceBnF dataCzech RepublicSpainIsraelOtherNARA\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Robotics&oldid=1247477220\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(web_documents)\n",
        "print(splits)\n",
        "print(f\"Your {len(web_documents)} documents have been split into {len(splits)} chunks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OnnWazGlTTi",
        "outputId": "609b61cd-5194-46bc-c9db-fae7a16f3816"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Design, construction, use, and application of robots\\nThis article may relate to a different subject or has undue weight on an aspect of the subject. Specifically, the article goes in too much detail on specific types of robot and includes product placement. Please help relocate relevant information and remove irrelevant content. (August 2024)\\n\\n\\nRoboticists with three Mars rover robots. Front and center is the flight spare for the first Mars rover, Sojourner, which landed on Mars in 1997 as part of the Mars Pathfinder Project. On the left is a Mars Exploration Rover (MER) test vehicle that is a working sibling to Spirit and Opportunity, which landed on Mars in 2004. On the right is a test rover for the Mars Science Laboratory, which landed Curiosity on Mars in 2012.\\n\\nRobotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]\\nWithin mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\\n\\n\\n\\nRobotics aspects[edit]\\nMechanical aspect\\nElectrical aspect\\nSoftware aspect\\nRobotics usually combines three aspects of design work to create robot systems:'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Mechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments.[2] The mechanical aspect of the robot is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function.\\nElectrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol-powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status), and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations)\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Software. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not be able to go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly structured, its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence, and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. A hybrid is a form of programming that incorporates both AI and RC functions in them.[3]\\nApplied robotics[edit]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Applied robotics[edit]\\nAs more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4]\\nCurrent and potential applications include:'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales.[5] In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[6]\\nAutonomous transport including airplane autopilot and self-driving cars\\nDomestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading[7] and flatbread baking.[8]\\nConstruction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[9]\\nAutomated mining.\\nSpace exploration, including Mars rovers.\\nEnergy applications including cleanup of nuclear contaminated areas[a]; and cleaning solar panel arrays.\\nMedical robots and Robot-assisted surgery designed and used in clinics.[11]\\nAgricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Agricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13]\\nFood processing. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts), Sally (salads),[14] salad or food bowl robots manufactured by Dexai (a Draper Laboratory spinoff, operating on military bases), and integrated food bowl assembly systems manufactured by Spyce Kitchen (acquired by Sweetgreen) and Silicon Valley startup Hyphen.[15] Other examples may include manufacturing technologies based on 3D Food Printing.\\nMilitary robots.\\nRobot sports for entertainment and education, including Robot combat, Autonomous racing, drone racing, and FIRST Robotics.\\nMechanical robotics areas[edit]\\nPower source[edit]\\nFurther information: Power supply and Energy storage\\nThe InSight lander with solar panels deployed in a cleanroom'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Mechanical robotics areas[edit]\\nPower source[edit]\\nFurther information: Power supply and Energy storage\\nThe InSight lander with solar panels deployed in a cleanroom\\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] \\nPotential power sources could be:'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='pneumatic (compressed gases)\\nSolar power (using the sun\\'s energy and converting it into electrical power)\\nhydraulics (liquids)\\nflywheel energy storage\\norganic garbage (through anaerobic digestion)\\nnuclear\\nActuation[edit]\\nMain article: Actuator\\nA robotic leg powered by air muscles \\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\\n\\nElectric motors[edit]\\nMain article: Electric motor\\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Linear actuators[edit]\\nMain article: Linear actuator\\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Series elastic actuators[edit]\\nSeries elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Air muscles[edit]\\nMain article: Pneumatic artificial muscles\\nPneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27]\\n\\nWire muscles[edit]\\nMain article: Shape memory alloy\\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29]\\n\\nElectroactive polymers[edit]\\nMain article: Electroactive polymers\\nEAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Piezo motors[edit]\\nMain article: Piezoelectric motor\\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[34] These motors are already available commercially and being used on some robots.[35][36]\\n\\nElastic nanotubes[edit]\\nFurther information: Carbon nanotube\\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10\\xa0J/cm3 for metal nanotubes. Human biceps could be replaced with an 8\\xa0mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Sensing[edit]\\nMain articles: Robotic sensing and Robotic sensors\\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Touch[edit]\\nMain article: Tactile sensor\\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[38][39] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Further information: Sensory-motor map\\nOther[edit]\\nOther common forms of sensing in robotics use lidar, radar, and sonar.[41] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\\n\\nMechanical grippers[edit]\\nOne of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[43] Hands that are of a mid-level complexity include the Delft hand.[44][45] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Suction end-effectors[edit]\\nSuction end-effectors, powered by vacuum generators, are very simple astrictive[46] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\\nSuction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\\n\\nGeneral purpose effectors[edit]\\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47] and the Schunk hand.[48] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[49]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Control robotics areas[edit]\\nPuppet Magnus, a robot-manipulated marionette with complex control systems\\nExperimental planar robot arm and sensor-based, open-architecture robot controller\\nRuBot II can manually resolve Rubik's cubes.\\nFurther information: Control system and Principles of motion sensing\\nThe mechanical structure of a robot must be controlled to perform tasks.[50] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[51] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot\\'s gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52]\\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[53] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[52] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[52] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Manipulation[edit]\\nKUKA industrial robot operating in a foundry\\nPuma, one of the first industrial robots\\nBaxter, a modern and versatile industrial robot developed by Rodney Brooks\\nLefty, first checker playing robot\\nFurther information: Mobile manipulator\\nA definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent\\'s control of its environment through selective contact\".[56]\\nRobots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[57] while the \"arm\" is referred to as a manipulator.[58] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59]\\n\\n\\nLocomotion[edit]\\nMain articles: Robot locomotion and Mobile robot\\nRolling robots[edit]\\nSegway in the Robot museum in Nagoya\\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Two-wheeled balancing robots[edit]\\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[60] Many different balancing robots have been designed.[61] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA\\'s Robonaut that has been mounted on a Segway.[62]\\n\\nOne-wheeled balancing robots[edit]\\nMain article: Self-balancing unicycle\\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University\\'s \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University\\'s \"BallIP\".[63] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Spherical orb robots[edit]\\nMain article: Spherical robot\\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66] or by rotating the outer shells of the sphere.[67][68] These have also been referred to as an orb bot[69] or a ball bot.[70][71]\\n\\nSix-wheeled robots[edit]\\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\\n\\nTracked robots[edit]\\nTracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA\\'s Urban Robot \"Urbie\".[72]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Walking robots[edit]\\nFurther information: Mantis the spider robot\\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"ZMP technique[edit]\\nMain article: Zero moment point\\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[76] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[77][78][79] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Hopping[edit]\\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[81] A quadruped was also demonstrated which could trot, run, pace, and bound.[82] For a full list of these robots, see the MIT Leg Lab Robots page.[83]\\n\\nDynamic balancing (controlled falling)[edit]\\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[84] This technique was recently demonstrated by Anybots' Dexter Robot,[85] which is so stable, it can even jump.[86] Another example is the TU Delft Flame.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Passive dynamics[edit]\\nMain article: Passive dynamics\\nPerhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88]\\n\\n\\n\\n\\nFlying[edit]\\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Biomimetic flying robots (BFRs)[edit]\\nA flapping wing BFR generating lift and thrust.\\nBFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[90] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\\nMammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91] Examples of bat inspired BFRs include Bat Bot[92] and the DALER.[93] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91]\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Dragonfly inspired BFR.\\nBird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94] An example of a raptor inspired BFR is the prototype by Savastano et al.[95] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8\\xa0kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96]\\nInsect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97] and a dragonfly inspired BFR is the prototype by Hu et al.[98] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[99] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Biologically-inspired flying robots[edit]\\nVisualization of entomopter flying on Mars (NASA)\\nA class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Snaking[edit]\\nTwo robot snakes. The left one has 64 motors (with 2 degrees of freedom per segment), the right one 10.\\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100] The Japanese ACM-R5 snake robot[101] can even navigate both on land and in water.[102]\\n\\nSkating[edit]\\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Capuchin, a climbing robot\\nClimbing[edit]\\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[105] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106] and Stickybot.[107]\\nChina\\'s Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Swimming (Piscine)[edit]\\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[108] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109] Notable examples are the Robotic Fish G9,[110] and Robot Tuna built to analyze and mathematically model thunniform motion.[111] The Aqua Penguin,[112] copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively.\\n\\n\\nRobotic Fish: iSplash-II\\nIn 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113] This build attained swimming speeds of 11.6BL/s (i.e. 3.7\\xa0m/s).[114] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Sailing[edit]\\nThe autonomous sailboat robot Vaimos\\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos.[116] Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\\n\\nComputational robotics areas[edit]\\nTOPIO, a humanoid robot, played ping pong at Tokyo IREX 2009.[117]\\nControl systems may also have varying levels of autonomy.\\n\\nDirect interaction is used for haptic or teleoperated devices, and the human has nearly complete control over the robot's motion.\\nOperator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them.[118]\\nAn autonomous robot may go without human interaction for extended periods of time . Higher levels of autonomy do not necessarily require more complex cognitive capabilities. For example, robots in assembly plants are completely autonomous but operate in a fixed pattern.\\nAnother classification takes into account the interaction between human control and the machine motions.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Teleoperation. A human controls each movement, each machine actuator change is specified by the operator.\\nSupervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators.\\nTask-level autonomy. The operator specifies only the task and the robot manages itself to complete it.\\nFull autonomy. The machine will create and complete all its tasks without human interaction.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Vision[edit]\\nMain article: Computer vision\\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\\nComputer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots\\' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Environmental interaction and navigation[edit]\\nMain articles: Robotic mapping and Robotic navigation\\nRadar, GPS, and lidar, are all combined to provide proper navigation and obstacle avoidance (vehicle developed for 2007 DARPA Urban Challenge).\\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2009) (Learn how and when to remove this message)'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Human-robot interaction[edit]\\nMain article: Human-robot interaction\\nKismet can produce a range of facial expressions.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[120] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Speech recognition[edit]\\nMain article: Speech recognition\\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[122] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[123] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125] With the help of artificial intelligence, machines nowadays can use people\\'s voice to identify their emotions such as satisfied or angry.[126]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Robotic voice[edit]\\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[127] making it necessary to develop the emotional component of robotic voice through various techniques.[128][129] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[130][131] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132] It was programmed to teach students in The Bronx, New York.[132]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Facial expression[edit]\\nFurther information: Emotion recognition\\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[134] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Gestures[edit]\\nFurther information: Gesture recognition\\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136] A great many systems have been developed to recognize human hand gestures.[137]\\n\\nProxemics[edit]\\nProxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Proxemics[edit]\\nProxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\\n\\nArtificial emotions[edit]\\nArtificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Personality[edit]\\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[139] Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Research robotics[edit]\\nFurther information: Areas of robotics\\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT\\'s cyberflora project, are almost wholly academic.\\nTo describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Dynamics and kinematics[edit]\\nFurther information: Kinematics and Dynamics (mechanics)\\nExternal videos How the BB-8 Sphero Toy Works\\nThe study of motion can be divided into kinematics and dynamics.[144] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Open source robotics[edit]\\nFurther information: Open source robotics\\nOpen source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Evolutionary robotics[edit]\\nEvolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145] and to explore the nature of evolution.[146] Because the process often requires many generations of robots to be simulated,[147] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[148] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]\\n\\nBionics and biomimetics[edit]\\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Swarm robotics[edit]\\nSwarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [119]\\n\\nQuantum computing[edit]\\nThere has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[149]\\n\\nOther research areas[edit]\\nNanorobots.\\nCobots (collaborative robots).[150]\\nAutonomous drones.\\nHigh temperature crucibles allow robotic systems to automate sample analysis.[151]\\nThe main venues for robotics research are the international conferences ICRA and IROS.\\n\\nHuman factors[edit]\\nEducation and training[edit]\\nMain article: Educational robotics\\nThe SCORBOT-ER 4u educational robot\\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[152] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[153] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Employment[edit]\\nA robot technician builds small all-terrain robots (courtesy: MobileRobots, Inc.).\\nMain article: Technological unemployment\\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[154] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[155] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[156] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[157]   The rise of robotics is thus often used as an argument for universal basic income.\\nAccording to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[158]'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Occupational safety and health implications[edit]\\nMain article: Workplace robotics safety\\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[159]\\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[160]\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\\nIn the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[161][162] aiming to protect employees from the risk of working with collaborative robots will have to be revised.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"User experience[edit]\\nGreat user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[163]\\nIt defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[164] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Careers[edit]\\nRobotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.    \\nRobotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"History[edit]\\nSee also: History of robots\\n\\n\\nDate\\n\\nSignificance\\n\\nRobot name\\n\\nInventor\\n\\n\\nc. 420 B.C.\\n\\nA wooden, steam-propelled bird, which was able to fly\\n\\nFlying pigeon\\n\\nArchytas of Tarentum\\n\\n\\nThird century B.C. and earlier\\n\\nOne of the earliest descriptions of automata appears in the Lie Zi text, on a much earlier encounter between King Mu of Zhou (1023–957 BC) and a mechanical engineer known as Yan Shi, an 'artificer'. The latter allegedly presented the king with a life-size, human-shaped figure of his mechanical handiwork.[165]\\n\\n\\n\\nYan Shi (Chinese: 偃师)\\n\\n\\nFirst century A.D. and earlier\\n\\nDescriptions of more than 100 machines and automata, including a fire engine, a wind organ, a coin-operated machine, and a steam-powered engine, in Pneumatica and Automata by Heron of Alexandria\\n\\n\\n\\nCtesibius, Philo of Byzantium, Heron of Alexandria, and others\\n\\n\\n1206\\n\\nCreated early humanoid automata, programmable automaton band[166]Robot band, hand-washing automaton,[167] automated moving peacocks[168]\\n\\n\\n\\nAl-Jazari\\n\\n\\n1495\\n\\nDesigns for a humanoid robot\\n\\nMechanical Knight\\n\\nLeonardo da Vinci\\n\\n\\n1560s\\n\\nClockwork Prayer that had machinal feet built under its robes that imitated walking. The robot's eyes, lips, and head all move in lifelike gestures.\\n\\nClockwork Prayer[citation needed]\\n\\nGianello della Torre\\n\\n\\n1738\\n\\nMechanical duck that was able to eat, flap its wings, and excrete\\n\\nDigesting Duck\\n\\nJacques de Vaucanson\\n\\n\\n1898\\n\\nNikola Tesla demonstrates the first radio-controlled vessel.\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='1738\\n\\nMechanical duck that was able to eat, flap its wings, and excrete\\n\\nDigesting Duck\\n\\nJacques de Vaucanson\\n\\n\\n1898\\n\\nNikola Tesla demonstrates the first radio-controlled vessel.\\n\\nTeleautomaton\\n\\nNikola Tesla\\n\\n\\n1903\\n\\nLeonardo Torres Quevedo presented the Telekino at the Paris Academy of Science, a radio-based control system with different operational states, for testing airships without risking human lives.[169] He conduct the initial test controlling a tricycle almost 100 feet away, being the first example of a radio-controlled unmanned ground vehicle.[170][171]\\n\\nTelekino\\n\\nLeonardo Torres Quevedo\\n\\n\\n1912\\n\\nLeonardo Torres Quevedo builds the first truly autonomous machine capable of playing chess. As opposed to the human-operated The Turk and Ajeeb, El Ajedrecista had an integrated automaton built to play chess without human guidance. It only played an endgame with three chess pieces, automatically moving a white king and a rook to checkmate the black king moved by a human opponent.[172][173]\\n\\nEl Ajedrecista\\n\\nLeonardo Torres Quevedo\\n\\n\\n1914'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='El Ajedrecista\\n\\nLeonardo Torres Quevedo\\n\\n\\n1914\\n\\nIn his paper Essays on Automatics published in 1914, Leonardo Torres Quevedo proposed a machine that makes \"judgments\" using sensors that capture information from the outside, parts that manipulate the outside world like arms, power sources such as batteries and air pressure, and most importantly, captured information and past information. It was defined as an organism that can control reactions in response to external information and adapt to changes in the environment to change its behavior.[174][175][176][177]\\n\\nEssays on Automatics\\n\\nLeonardo Torres Quevedo\\n\\n\\n1921\\n\\nFirst fictional automatons called \"robots\" appear in the play R.U.R.\\n\\nRossum\\'s Universal Robots\\n\\nKarel Čapek\\n\\n\\n1930s\\n\\nHumanoid robot exhibited at the 1939 and 1940 World\\'s Fairs\\n\\nElektro\\n\\nWestinghouse Electric Corporation\\n\\n\\n1946\\n\\nFirst general-purpose digital computer\\n\\nWhirlwind\\n\\nMultiple people\\n\\n\\n1948\\n\\nSimple robots exhibiting biological behaviors[178]\\n\\nElsie and Elmer\\n\\nWilliam Grey Walter\\n\\n\\n1948\\n\\nFormulation of principles of cybernetics\\n\\ncybernetics\\n\\nNorbert Wiener\\n\\n\\n1956\\n\\nFirst commercial robot, from the Unimation company founded by George Devol and Joseph Engelberger, based on Devol\\'s patents[179]\\n\\nUnimate\\n\\nGeorge Devol\\n\\n\\n1961\\n\\nFirst installed industrial robot.  The first digitally operated and programmable robot, Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them.\\n\\nUnimate\\n\\nGeorge Devol\\n\\n\\n1967 to 1972'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Unimate\\n\\nGeorge Devol\\n\\n\\n1967 to 1972\\n\\nFirst full-scale humanoid intelligent robot,[180][181] and first android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with its hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes, and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.[182][183][184]\\n\\nWABOT-1\\n\\nWaseda University\\n\\n\\n1973\\n\\nFirst industrial robot with six electromechanically driven axes[185][186]\\n\\nFamulus\\n\\nKUKA Robot Group\\n\\n\\n1974\\n\\nThe world's first microcomputer controlled electric industrial robot, IRB 6 from ASEA, was delivered to a small mechanical engineering company in southern Sweden. The design of this robot had been patented in 1972.\\n\\nIRB 6\\n\\nABB Robot Group\\n\\n\\n1975\\n\\nProgrammable universal manipulation arm, a Unimation product\\n\\nPUMA\\n\\nVictor Scheinman\\n\\n\\n1978\\n\\nThe first object-level robot programming language, RAPT, allowing robots to handle variations in object position, shape, and sensor noise.[187]\\n\\nFreddy I and II\\n\\nPatricia Ambler and Robin Popplestone\\n\\n\\n1983\\n\\nFirst multitasking, the parallel programming language used for robot control. It was the Event Driven Language (EDL) on the IBM/Series/1 process computer, with the implementation of both inter-process communication (WAIT/POST) and mutual exclusion (ENQ/DEQ) mechanisms for robot control.[188]\\n\\nADRIEL I\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='ADRIEL I\\n\\nStevo Bozinovski and Mihail Sestakov\\n\\n\\nSee also[edit]\\n\\nArtificial intelligence\\nAutonomous robot\\nCloud robotics\\nCognitive robotics\\nEvolutionary robotics\\nFog robotics\\nGlossary of robotics\\nIndex of robotics articles\\nMechatronics\\nMulti-agent system\\nOutline of robotics\\nQuantum robotics\\nRoboethics\\nRobot rights\\nRobotic art\\nRobotic governance\\nSelf-reconfiguring modular robot\\nSoft robotics\\nTelerobotics\\n\\nNotes[edit]\\n\\n\\n^ One database, developed by the United States Department of Energy, contains information on almost 500 existing robotic technologies.[10]\\n\\n\\nReferences[edit]\\n\\n\\n^ \"German National Library\". International classification system of the German National Library (GND). Archived from the original on 2020-08-19.\\n\\n^ \"Origami-Inspired Robots Can Sense, Analyze and Act in Challenging Environments\". UCLA. Retrieved 2023-04-10.\\n\\n^ Raj, Aditi (26 August 2024). \"AI & Robotics: The Role of AI in Robots\". Retrieved 2024-08-29.\\n\\n^ Hunt, V. Daniel (1985). \"Smart Robots\". Smart Robots: A Handbook of Intelligent Robotic Systems. Chapman and Hall. p.\\xa0141. ISBN\\xa0978-1-4613-2533-8. Archived from the original on 2023-03-15. Retrieved 2018-12-04.\\n\\n^ \"Robot density rises globally\". Robotic Industries Association. 8 February 2018. Archived from the original on 2020-11-23. Retrieved 2018-12-03.\\n\\n^ Pinto, Jim (1 October 2003). \"Fully automated factories approach reality\". Automation World. Archived from the original on 2011-10-01. Retrieved 2018-12-03.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Pinto, Jim (1 October 2003). \"Fully automated factories approach reality\". Automation World. Archived from the original on 2011-10-01. Retrieved 2018-12-03.\\n\\n^ Eyre, Michael (12 September 2014). \"\\'Boris\\' the robot can load up dishwasher\". BBC News. Archived from the original on 2020-12-21. Retrieved 2018-12-03.\\n\\n^ Corner, Stuart (23 November 2017). \"AI-driven robot makes \\'perfect\\' flatbread\". iothub.com.au. Archived from the original on 2020-11-24. Retrieved 2018-12-03.\\n\\n^ Pollock, Emily (7 June 2018). \"Construction Robotics Industry Set to Double by 2023\". engineering.com. Archived from the original on 2020-08-07. Retrieved 2018-12-03.\\n\\n^ \"Technology Advanced Search\". D&D Knowledge Management Information Tool. Archived from the original on 2020-08-06.\\n\\n^ Arámbula Cosío, F.; Hibberd, R. D.; Davies, B. L. (July 1997). \"Electromagnetic compatibility aspects of active robotic systems for surgery: the robotic prostatectomy experience\". Medical and Biological Engineering and Computing. 35 (4): 436–440. doi:10.1007/BF02534105. ISSN\\xa01741-0444. PMID\\xa09327627. S2CID\\xa021479700.\\n\\n^ Grift, Tony E. (2004). \"Agricultural Robotics\". University of Illinois at Urbana–Champaign. Archived from the original on 2007-05-04. Retrieved 2018-12-03.\\n\\n^ Thomas, Jim (1 November 2017). \"How corporate giants are automating the farm\". New Internationalist. Archived from the original on 2021-01-10. Retrieved 2018-12-03.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Thomas, Jim (1 November 2017). \"How corporate giants are automating the farm\". New Internationalist. Archived from the original on 2021-01-10. Retrieved 2018-12-03.\\n\\n^ Kolodny, Lora (4 July 2017). \"Robots are coming to a burger joint near you\". CNBC. Archived from the original on 2020-12-05. Retrieved 2018-12-03.\\n\\n^ Scott Kirsner (27 January 2023). \"Robots in the kitchen? Local engineers are making it a reality\". The Boston Globe.\\n\\n^ Dowling, Kevin. \"Power Sources for Small Robots\" (PDF). Carnegie Mellon University. Archived (PDF) from the original on 2020-11-25. Retrieved 2012-05-11.\\n\\n^ Roozing, Wesley; Li, Zhibin; Tsagarakis, Nikos; Caldwell, Darwin (2016). \"Design Optimisation and Control of Compliant Actuation Arrangements in Articulated Robots for Improved Energy Efficiency\". IEEE Robotics and Automation Letters. 1 (2): 1110–1117. doi:10.1109/LRA.2016.2521926. S2CID\\xa01940410.\\n\\n^ Pratt, G.A.; Williamson, M.M. (1995). \"Series elastic actuators\". Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human-Robot Interaction and Cooperative Robots. Vol.\\xa01. pp.\\xa0399–406. doi:10.1109/IROS.1995.525827. hdl:1721.1/36966. ISBN\\xa00-8186-7108-4. S2CID\\xa017120394.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Furnémont, Raphaël; Mathijssen, Glenn; Verstraten, Tom; Lefeber, Dirk; Vanderborght, Bram (27 January 2016). \"Bi-directional series-parallel elastic actuator and overlap of the actuation layers\" (PDF). Bioinspiration & Biomimetics. 11 (1): 016005. Bibcode:2016BiBi...11a6005F. doi:10.1088/1748-3190/11/1/016005. PMID\\xa026813145. S2CID\\xa037031990. Archived (PDF) from the original on 2022-10-01. Retrieved 2023-03-15.\\n\\n^ Pratt, Jerry E.; Krupp, Benjamin T. (2004). \"Series Elastic Actuators for legged robots\". In Gerhart, Grant R; Shoemaker, Chuck M; Gage, Douglas W (eds.). Unmanned Ground Vehicle Technology VI. Vol.\\xa05422. pp.\\xa0135–144. doi:10.1117/12.548000. S2CID\\xa016586246.\\n\\n^ Li, Zhibin; Tsagarakis, Nikos; Caldwell, Darwin (2013). \"Walking Pattern Generation for a Humanoid Robot with Compliant Joints\". Autonomous Robots. 35 (1): 1–14. doi:10.1007/s10514-013-9330-7. S2CID\\xa0624563.\\n\\n^ Colgate, J. Edward (1988). The control of dynamically interacting systems (Thesis). hdl:1721.1/14380.\\n\\n^ Calanca, Andrea; Muradore, Riccardo; Fiorini, Paolo (November 2017). \"Impedance control of series elastic actuators: Passivity and acceleration-based control\". Mechatronics. 47: 37–48. doi:10.1016/j.mechatronics.2017.08.010.\\n\\n^ Tosun, Fatih Emre; Patoglu, Volkan (June 2020). \"Necessary and Sufficient Conditions for the Passivity of Impedance Rendering With Velocity-Sourced Series Elastic Actuation\". IEEE Transactions on Robotics. 36 (3): 757–772. doi:10.1109/TRO.2019.2962332. S2CID\\xa0212907787.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ www.imagesco.com, Images SI Inc -. \"Air Muscle actuators, going further, page 6\". Archived from the original on 2020-11-14. Retrieved 2010-05-24.\\n\\n^ \"Air Muscles\". Shadow Robot. Archived from the original on 2007-09-27.\\n\\n^ Tondu, Bertrand (2012). \"Modelling of the McKibben artificial muscle: A review\". Journal of Intelligent Material Systems and Structures. 23 (3): 225–253. doi:10.1177/1045389X11435435. S2CID\\xa0136854390.\\n\\n^ \"TALKING ELECTRONICS Nitinol Page-1\". Talkingelectronics.com. Archived from the original on 2020-01-18. Retrieved 2010-11-27.\\n\\n^ \"lf205, Hardware: Building a Linux-controlled walking robot\". Ibiblio.org. 1 November 2001. Archived from the original on 2016-03-03. Retrieved 2010-11-27.\\n\\n^ \"WW-EAP and Artificial Muscles\". Eap.jpl.nasa.gov. Archived from the original on 2017-01-20. Retrieved 2010-11-27.\\n\\n^ \"Empa – a117-2-eap\". Empa.ch. Archived from the original on 2015-09-24. Retrieved 2010-11-27.\\n\\n^ \"Electroactive Polymers (EAP) as Artificial Muscles (EPAM) for Robot Applications\". Hizook. Archived from the original on 2020-08-06. Retrieved 2010-11-27.\\n\\n^ \"Piezo LEGS – -09-26\". Archived from the original on 2008-01-30. Retrieved 2007-10-28.\\n\\n^ \"Squiggle Motors: Overview\". Archived from the original on 2007-10-07. Retrieved 2007-10-08.\\n\\n^ Nishibori; et\\xa0al. (2003). \"Robot Hand with Fingers Using Vibration-Type Ultrasonic Motors (Driving Characteristics)\". Journal of Robotics and Mechatronics. 15 (6): 588–595. doi:10.20965/jrm.2003.p0588.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Otake, Mihoko; Kagami, Yoshiharu; Ishikawa, Kohei; Inaba, Masayuki; Inoue, Hirochika (6 April 2001). Wilson, Alan R.; Asanuma, Hiroshi (eds.). \"Shape design of gel robots made of electroactive polymer gel\". Smart Materials. 4234: 194–202. Bibcode:2001SPIE.4234..194O. doi:10.1117/12.424407. S2CID\\xa030357330.\\n\\n^ Madden, John D. (16 November 2007). \"Mobile Robots: Motor Challenges and Materials Solutions\". Science. 318 (5853): 1094–1097. Bibcode:2007Sci...318.1094M. CiteSeerX\\xa010.1.1.395.4635. doi:10.1126/science.1146351. PMID\\xa018006737. S2CID\\xa052827127.\\n\\n^ \"Syntouch LLC: BioTac(R) Biomimetic Tactile Sensor Array\". Archived from the original on 2009-10-03. Retrieved 2009-08-10.\\n\\n^ Wettels, Nicholas; Santos, Veronica J.; Johansson, Roland S.; Loeb, Gerald E. (January 2008). \"Biomimetic Tactile Sensor Array\". Advanced Robotics. 22 (8): 829–849. doi:10.1163/156855308X314533. S2CID\\xa04594917.\\n\\n^ \"What is The SmartHand?\". SmartHand Project. Archived from the original on 2015-03-03. Retrieved 2011-02-04.\\n\\n^ a b Arreguin, Juan (2008). Automation and Robotics. Vienna, Austria: I-Tech and Publishing.\\n\\n^ \"Annotated Mythbusters: Episode 78: Ninja Myths – Walking on Water, Catching a Sword, Catching an Arrow\". Archived from the original on 2020-11-12. Retrieved 2010-02-13. (Discovery Channel\\'s Mythbusters making mechanical gripper from the chain and metal wire)\\n\\n^ \"Robonaut hand\". Archived from the original on 2020-02-22. Retrieved 2011-11-21.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Robonaut hand\". Archived from the original on 2020-02-22. Retrieved 2011-11-21.\\n\\n^ \"Delft hand\". TU Delft. Archived from the original on 2012-02-03. Retrieved 2011-11-21.\\n\\n^ M&C. \"TU Delft ontwikkelt goedkope, voorzichtige robothand\". Archived from the original on 2017-03-13. Retrieved 2011-11-21.\\n\\n^ \"astrictive definition – English definition dictionary – Reverso\". Archived from the original on 2020-04-30. Retrieved 2008-01-06.\\n\\n^ Tijsma, H.A.; Liefhebber, F.; Herder, J.L. (2005). \"Evaluation of New User Interface Features for the MANUS Robot Arm\". 9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005. pp.\\xa0258–263. doi:10.1109/ICORR.2005.1501097. ISBN\\xa00-7803-9003-2. S2CID\\xa036445389.\\n\\n^ Allcock, Andrew (2006). \"Anthropomorphic hand is almost human\". Machinery. Archived from the original on 2007-09-28. Retrieved 2007-10-17.\\n\\n^ \"Welcome\". Archived (PDF) from the original on 2013-05-10. Retrieved 2007-10-28.\\n\\n^ a b c Corke, Peter (2017). Robotics, Vision and Control. Springer Tracts in Advanced Robotics. Vol.\\xa0118. doi:10.1007/978-3-319-54413-7. ISBN\\xa0978-3-319-54412-0. ISSN\\xa01610-7438. Archived from the original on 2022-10-20. Retrieved 2023-03-15.\\n\\n^ a b c Lee, K. S. Fu, Ralph Gonzalez, C S. G. (1987). Robotics: Control Sensing. Vis. McGraw-Hill. ISBN\\xa0978-0-07-026510-3. Archived from the original on 2023-03-15. Retrieved 2023-03-15.{{cite book}}:  CS1 maint: multiple names: authors list (link)'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ a b c d Short, Michael; Burn, Kevin (1 April 2011). \"A generic controller architecture for intelligent robotic systems\". Robotics and Computer-Integrated Manufacturing. 27 (2): 292–305. doi:10.1016/j.rcim.2010.07.013. ISSN\\xa00736-5845.\\n\\n^ Ray, Partha Pratim (2016). \"Internet of Robotic Things: Concept, Technologies, and Challenges\". IEEE Access. 4: 9489–9500. Bibcode:2016IEEEA...4.9489R. doi:10.1109/ACCESS.2017.2647747. ISSN\\xa02169-3536. S2CID\\xa09273802.\\n\\n^ a b Burn, K.; Short, M.; Bicker, R. (July 2003). \"Adaptive and Nonlinear Fuzzy Force Control Techniques Applied to Robots Operating in Uncertain Environments\". Journal of Robotic Systems. 20 (7): 391–400. doi:10.1002/rob.10093. ISSN\\xa00741-2223. Archived from the original on 2022-11-26. Retrieved 2023-03-15.\\n\\n^ Burn, Kevin; Home, Geoffrey (1 May 2008). \"Environment classification using Kohonen self-organizing maps\". Expert Systems. 25 (2): 98–114. doi:10.1111/j.1468-0394.2008.00441.x. ISSN\\xa00266-4720. S2CID\\xa033369232.\\n\\n^ Mason, Matthew T. (2001). Mechanics of Robotic Manipulation. doi:10.7551/mitpress/4527.001.0001. ISBN\\xa09780262256629. S2CID\\xa05260407.\\n\\n^ \"What is a robotic end-effector?\". ATI Industrial Automation. 2007. Archived from the original on 2020-12-17. Retrieved 2007-10-16.\\n\\n^ Crane, Carl D.; Joseph Duffy (1998). Kinematic Analysis of Robot Manipulators. Cambridge University Press. ISBN\\xa0978-0-521-57063-3. Archived from the original on 2020-04-02. Retrieved 2007-10-16.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Crane, Carl D.; Joseph Duffy (1998). Kinematic Analysis of Robot Manipulators. Cambridge University Press. ISBN\\xa0978-0-521-57063-3. Archived from the original on 2020-04-02. Retrieved 2007-10-16.\\n\\n^ G.J. Monkman, S. Hesse, R. Steinmann & H. Schunk (2007). Robot Grippers. Berlin: Wiley\\n\\n^ \"T.O.B.B\". Mtoussaint.de. Archived from the original on 2020-07-08. Retrieved 2010-11-27.\\n\\n^ \"nBot, a two wheel balancing robot\". Geology.heroy.smu.edu. Archived from the original on 2021-01-26. Retrieved 2010-11-27.\\n\\n^ \"ROBONAUT Activity Report\". NASA. 2004. Archived from the original on 2007-08-20. Retrieved 2007-10-20.\\n\\n^ Guizzo, Erico (29 April 2010). \"A Robot That Balances on a Ball\". IEEE Spectrum. Archived from the original on 2023-02-10. Retrieved 2023-03-15.\\n\\n^ \"Carnegie Mellon Researchers Develop New Type of Mobile Robot That Balances and Moves on a Ball Instead of Legs or Wheels\" (Press release). Carnegie Mellon. 9 August 2006. Archived from the original on 2007-06-09. Retrieved 2007-10-20.\\n\\n^ \"Spherical Robot Can Climb Over Obstacles\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27.\\n\\n^ \"Rotundus\". Rotundus.se. Archived from the original on 2011-08-26. Retrieved 2010-11-27.\\n\\n^ \"OrbSwarm Gets A Brain\". BotJunkie. 11 July 2007. Archived from the original on 2012-05-16. Retrieved 2010-11-27.\\n\\n^ \"Rolling Orbital Bluetooth Operated Thing\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Rolling Orbital Bluetooth Operated Thing\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27.\\n\\n^ \"Swarm\". Orbswarm.com. Archived from the original on 2021-01-26. Retrieved 2010-11-27.\\n\\n^ \"The Ball Bot\\xa0: Johnnytronic@Sun\". Blogs.sun.com. Archived from the original on 2011-08-24. Retrieved 2010-11-27.\\n\\n^ \"Senior Design Projects | College of Engineering & Applied Science| University of Colorado at Boulder\". Engineering.colorado.edu. 30 April 2008. Archived from the original on 2011-07-23. Retrieved 2010-11-27.\\n\\n^ \"JPL Robotics: System: Commercial Rovers\". Archived from the original on 2006-06-15.\\n\\n^ \"AMBER Lab\". Archived from the original on 2020-11-25. Retrieved 2012-01-23.\\n\\n^ \"Micromagic Systems Robotics Lab\". Archived from the original on 2017-06-01. Retrieved 2009-04-29.\\n\\n^ \"AMRU-5 hexapod robot\" (PDF). Archived (PDF) from the original on 2016-08-17. Retrieved 2009-04-29.\\n\\n^ \"Achieving Stable Walking\". Honda Worldwide. Archived from the original on 2011-11-08. Retrieved 2007-10-22.\\n\\n^ \"Funny Walk\". Pooter Geek. 28 December 2004. Archived from the original on 2011-09-28. Retrieved 2007-10-22.\\n\\n^ \"ASIMO\\'s Pimp Shuffle\". Popular Science. 9 January 2007. Archived from the original on 2011-07-24. Retrieved 2007-10-22.\\n\\n^ \"Robot Shows Prime Minister How to Loosen Up > > A drunk robot?\". The Temple of VTEC – Honda and Acura Enthusiasts Online Forums. 25 August 2003. Archived from the original on 2020-04-30.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Robot Shows Prime Minister How to Loosen Up > > A drunk robot?\". The Temple of VTEC – Honda and Acura Enthusiasts Online Forums. 25 August 2003. Archived from the original on 2020-04-30.\\n\\n^ \"3D One-Leg Hopper (1983–1984)\". MIT Leg Laboratory. Archived from the original on 2018-07-25. Retrieved 2007-10-22.\\n\\n^ \"3D Biped (1989–1995)\". MIT Leg Laboratory. Archived from the original on 2011-09-26. Retrieved 2007-10-28.\\n\\n^ \"Quadruped (1984–1987)\". MIT Leg Laboratory. Archived from the original on 2011-08-23. Retrieved 2007-10-28.\\n\\n^ \"MIT Leg Lab Robots- Main\". Archived from the original on 2020-08-07. Retrieved 2007-10-28.\\n\\n^ \"About the Robots\". Anybots. Archived from the original on 2007-09-09. Retrieved 2007-10-23.\\n\\n^ \"Anything, Anytime, Anywhere\". Anybots. Archived from the original on 2007-10-27. Retrieved 2007-10-23.\\n\\n^ \"Dexter Jumps video\". YouTube. 1 March 2007. Archived from the original on 2021-10-30. Retrieved 2007-10-23.\\n\\n^ Collins, Steve; Ruina, Andy; Tedrake, Russ; Wisse, Martijn (18 February 2005). \"Efficient Bipedal Robots Based on Passive-Dynamic Walkers\". Science. 307 (5712): 1082–1085. Bibcode:2005Sci...307.1082C. doi:10.1126/science.1107799. PMID\\xa015718465. S2CID\\xa01315227.\\n\\n^ Collins, S.H.; Ruina, A. (2005). \"A Bipedal Walking Robot with Efficient and Human-Like Gait\". Proceedings of the 2005 IEEE International Conference on Robotics and Automation. pp.\\xa01983–1988. doi:10.1109/ROBOT.2005.1570404. ISBN\\xa00-7803-8914-X. S2CID\\xa015145353.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Testing the Limits\" (PDF). Boeing. p.\\xa029. Archived (PDF) from the original on 2018-12-15. Retrieved 2008-04-09.\\n\\n^ Zhang, Jun; Zhao, Ning; Qu, Feiyang (15 November 2022). \"Bio-inspired flapping wing robots with foldable or deformable wings: a review\". Bioinspiration & Biomimetics. 18 (1): 011002. doi:10.1088/1748-3190/ac9ef5. ISSN\\xa01748-3182. PMID\\xa036317380. S2CID\\xa0253246037.\\n\\n^ a b c Shin, Won Dong; Park, Jaejun; Park, Hae-Won (1 September 2019). \"Development and experiments of a bio-inspired robot with multi-mode in aerial and terrestrial locomotion\". Bioinspiration & Biomimetics. 14 (5): 056009. Bibcode:2019BiBi...14e6009S. doi:10.1088/1748-3190/ab2ab7. ISSN\\xa01748-3182. PMID\\xa031212268. S2CID\\xa0195066183.\\n\\n^ Ramezani, Alireza; Shi, Xichen; Chung, Soon-Jo; Hutchinson, Seth (May 2016). \"Bat Bot (B2), a biologically inspired flying machine\". 2016 IEEE International Conference on Robotics and Automation (ICRA). Stockholm, Sweden: IEEE. pp.\\xa03219–3226. doi:10.1109/ICRA.2016.7487491. ISBN\\xa0978-1-4673-8026-3. S2CID\\xa08581750.\\n\\n^ a b Daler, Ludovic; Mintchev, Stefano; Stefanini, Cesare; Floreano, Dario (19 January 2015). \"A bioinspired multi-modal flying and walking robot\". Bioinspiration & Biomimetics. 10 (1): 016005. Bibcode:2015BiBi...10a6005D. doi:10.1088/1748-3190/10/1/016005. ISSN\\xa01748-3190. PMID\\xa025599118. S2CID\\xa011132948.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ a b Kilian, Lukas; Shahid, Farzeen; Zhao, Jing-Shan; Nayeri, Christian Navid (1 July 2022). \"Bioinspired morphing wings: mechanical design and wind tunnel experiments\". Bioinspiration & Biomimetics. 17 (4): 046019. Bibcode:2022BiBi...17d6019K. doi:10.1088/1748-3190/ac72e1. ISSN\\xa01748-3182. PMID\\xa035609562. S2CID\\xa0249045806.\\n\\n^ Savastano, E.; Perez-Sanchez, V.; Arrue, B.C.; Ollero, A. (July 2022). \"High-Performance Morphing Wing for Large-Scale Bio-Inspired Unmanned Aerial Vehicles\". IEEE Robotics and Automation Letters. 7 (3): 8076–8083. doi:10.1109/LRA.2022.3185389. ISSN\\xa02377-3766. S2CID\\xa0250008824.\\n\\n^ Grant, Daniel T.; Abdulrahim, Mujahid; Lind, Rick (June 2010). \"Flight Dynamics of a Morphing Aircraft Utilizing Independent Multiple-Joint Wing Sweep\". International Journal of Micro Air Vehicles. 2 (2): 91–106. doi:10.1260/1756-8293.2.2.91. ISSN\\xa01756-8293. S2CID\\xa0110577545.\\n\\n^ Phan, Hoang Vu; Park, Hoon Cheol (4 December 2020). \"Mechanisms of collision recovery in flying beetles and flapping-wing robots\". Science. 370 (6521): 1214–1219. Bibcode:2020Sci...370.1214P. doi:10.1126/science.abd3285. ISSN\\xa00036-8075. PMID\\xa033273101. S2CID\\xa0227257247.\\n\\n^ Hu, Zheng; McCauley, Raymond; Schaeffer, Steve; Deng, Xinyan (May 2009). \"Aerodynamics of dragonfly flight and robotic design\". 2009 IEEE International Conference on Robotics and Automation. pp.\\xa03061–3066. doi:10.1109/ROBOT.2009.5152760. ISBN\\xa0978-1-4244-2788-8. S2CID\\xa012291429.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Balta, Miquel; Deb, Dipan; Taha, Haithem E (26 October 2021). \"Flow visualization and force measurement of the clapping effect in bio-inspired flying robots\". Bioinspiration & Biomimetics. 16 (6): 066020. Bibcode:2021BiBi...16f6020B. doi:10.1088/1748-3190/ac2b00. ISSN\\xa01748-3182. PMID\\xa034584023. S2CID\\xa0238217893.\\n\\n^ Miller, Gavin. \"Introduction\". snakerobots.com. Archived from the original on 2011-08-17. Retrieved 2007-10-22.\\n\\n^ \"ACM-R5\". Archived from the original on 2011-10-11.\\n\\n^ \"Swimming snake robot (commentary in Japanese)\". Archived from the original on 2012-02-08. Retrieved 2007-10-28.\\n\\n^ \"Commercialized Quadruped Walking Vehicle \"TITAN VII\"\". Hirose Fukushima Robotics Lab. Archived from the original on 2007-11-06. Retrieved 2007-10-23.\\n\\n^ Pachal, Peter (23 January 2007). \"Plen, the robot that skates across your desk\". SCI FI Tech. Archived from the original on 2007-10-11.\\n\\n^ Capuchin on YouTube\\n\\n^ Wallbot on YouTube\\n\\n^ Stanford University: Stickybot on YouTube\\n\\n^ Sfakiotakis, M.; Lane, D.M.; Davies, J.B.C. (April 1999). \"Review of fish swimming modes for aquatic locomotion\". IEEE Journal of Oceanic Engineering. 24 (2): 237–252. Bibcode:1999IJOE...24..237S. CiteSeerX\\xa010.1.1.459.8614. doi:10.1109/48.757275. S2CID\\xa017226211.\\n\\n^ Richard Mason. \"What is the market for robot fish?\". Archived from the original on 2009-07-04.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Richard Mason. \"What is the market for robot fish?\". Archived from the original on 2009-07-04.\\n\\n^ \"Robotic fish powered by Gumstix PC and PIC\". Human Centred Robotics Group at Essex University. Archived from the original on 2011-08-14. Retrieved 2007-10-25.\\n\\n^ Witoon Juwarahawong. \"Fish Robot\". Institute of Field Robotics. Archived from the original on 2007-11-04. Retrieved 2007-10-25.\\n\\n^ \"Festo - AquaPenguin\". 17 April 2009 – via YouTube.\\n\\n^ \"High-Speed Robotic Fish\". iSplash-Robotics. Archived from the original on 2020-03-11. Retrieved 2017-01-07.\\n\\n^ \"iSplash-II: Realizing Fast Carangiform Swimming to Outperform a Real Fish\" (PDF). Robotics Group at Essex University. Archived from the original (PDF) on 2015-09-30. Retrieved 2015-09-29.\\n\\n^ \"iSplash-I: High Performance Swimming Motion of a Carangiform Robotic Fish with Full-Body Coordination\" (PDF). Robotics Group at Essex University. Archived from the original (PDF) on 2015-09-30. Retrieved 2015-09-29.\\n\\n^ Jaulin, Luc; Le Bars, Fabrice (February 2013). \"An Interval Approach for Stability Analysis: Application to Sailboat Robotics\". IEEE Transactions on Robotics. 29 (1): 282–287. CiteSeerX\\xa010.1.1.711.7180. doi:10.1109/TRO.2012.2217794. S2CID\\xa04977937.\\n\\n^ \"A Ping-Pong-Playing Terminator\". Popular Science. Archived from the original on 2021-01-22. Retrieved 2010-12-19.\\n\\n^ \"Synthiam Exosphere combines AI, human operators to train robots\". The Robot Report. Archived from the original on 2020-10-06. Retrieved 2020-04-29.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Synthiam Exosphere combines AI, human operators to train robots\". The Robot Report. Archived from the original on 2020-10-06. Retrieved 2020-04-29.\\n\\n^ a b Kagan, Eugene; Ben-Gal, Irad (2015). Search and foraging:individual motion and swarm dynamics. Chapman and Hall/CRC. ISBN\\xa09781482242102. Archived from the original on 2023-03-15. Retrieved 2020-08-26.\\n\\n^ Banks, Jaime (2020). \"Optimus Primed: Media Cultivation of Robot Mental Models and Social Judgments\". Frontiers in Robotics and AI. 7: 62. doi:10.3389/frobt.2020.00062. PMC\\xa07805817. PMID\\xa033501230.\\n\\n^ a b Wullenkord, Ricarda; Fraune, Marlena R.; Eyssel, Friederike; Sabanovic, Selma (2016). \"Getting in Touch: How imagined, actual, and physical contact affect evaluations of robots\". 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). pp.\\xa0980–985. doi:10.1109/ROMAN.2016.7745228. ISBN\\xa0978-1-5090-3929-6. S2CID\\xa06305599.\\n\\n^ Norberto Pires, J. (December 2005). \"Robot-by-voice: experiments on commanding an industrial robot using the human voice\". Industrial Robot. 32 (6): 505–511. doi:10.1108/01439910510629244.\\n\\n^ \"Survey of the State of the Art in Human Language Technology: 1.2: Speech Recognition\". Archived from the original on 2007-11-11.\\n\\n^ Fournier, Randolph Scott; Schmidt, B. June (1995). \"Voice input technology: Learning style and attitude toward its use\". Delta Pi Epsilon Journal. 37 (1): 1–12. ProQuest\\xa01297783046.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Fournier, Randolph Scott; Schmidt, B. June (1995). \"Voice input technology: Learning style and attitude toward its use\". Delta Pi Epsilon Journal. 37 (1): 1–12. ProQuest\\xa01297783046.\\n\\n^ \"History of Speech & Voice Recognition and Transcription Software\". Dragon Naturally Speaking. Archived from the original on 2015-08-13. Retrieved 2007-10-27.\\n\\n^ Cheng Lin, Kuan; Huang, Tien-Chi; Hung, Jason C.; Yen, Neil Y.; Ju Chen, Szu (7 June 2013). \"Facial emotion recognition towards affective computing-based learning\". Library Hi Tech. 31 (2): 294–307. doi:10.1108/07378831311329068.\\n\\n^ Walters, M. L.; Syrdal, D. S.; Koay, K. L.; Dautenhahn, K.; Te Boekhorst, R. (2008). \"Human approach distances to a mechanical-looking robot with different robot voice styles\". RO-MAN 2008 - the 17th IEEE International Symposium on Robot and Human Interactive Communication. pp.\\xa0707–712. doi:10.1109/ROMAN.2008.4600750. ISBN\\xa0978-1-4244-2212-8. S2CID\\xa08653718.\\n\\n^ Pauletto, Sandra; Bowles, Tristan (2010). \"Designing the emotional content of a robotic speech signal\". Proceedings of the 5th Audio Mostly Conference on a Conference on Interaction with Sound - AM \\'10. pp.\\xa01–8. doi:10.1145/1859799.1859804. ISBN\\xa0978-1-4503-0046-9. S2CID\\xa030423778.\\n\\n^ Bowles, Tristan; Pauletto, Sandra (2010). Emotions in the Voice: Humanising a Robotic Voice (PDF). Proceedings of the 7th Sound and Music Computing Conference. Barcelona. Archived (PDF) from the original on 2023-02-10. Retrieved 2023-03-15.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"World of 2-XL: Leachim\". www.2xlrobot.com. Archived from the original on 2020-07-05. Retrieved 2019-05-28.\\n\\n^ \"The Boston Globe from Boston, Massachusetts on June 23, 1974 · 132\". Newspapers.com. 23 June 1974. Archived from the original on 2020-01-10. Retrieved 2019-05-28.\\n\\n^ a b \"cyberneticzoo.com - Page 135 of 194 - a history of cybernetic animals and early robots\". cyberneticzoo.com. Archived from the original on 2020-08-06. Retrieved 2019-05-28.\\n\\n^ \"Frubber facial expressions\". Archived from the original on 2009-02-07.\\n\\n^ \"Best Inventions of 2008 – TIME\". Time. 29 October 2008. Archived from the original on 2008-11-02 – via www.time.com.\\n\\n^ \"Kismet: Robot at MIT\\'s AI Lab Interacts With Humans\". Sam Ogden. Archived from the original on 2007-10-12. Retrieved 2007-10-28.\\n\\n^ Waldherr, Stefan; Romero, Roseli; Thrun, Sebastian (1 September 2000). \"A Gesture Based Interface for Human-Robot Interaction\". Autonomous Robots. 9 (2): 151–173. doi:10.1023/A:1008918401478. S2CID\\xa01980239.\\n\\n^ Li, Ling Hua; Du, Ji Fang (December 2012). \"Visual Based Hand Gesture Recognition Systems\". Applied Mechanics and Materials. 263–266: 2422–2425. Bibcode:2012AMM...263.2422L. doi:10.4028/www.scientific.net/AMM.263-266.2422. S2CID\\xa062744240.\\n\\n^ \"Armenian Robin the Robot to comfort kids at U.S. clinics starting July\". Public Radio of Armenia. Archived from the original on 2021-05-13. Retrieved 2021-05-13.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Armenian Robin the Robot to comfort kids at U.S. clinics starting July\". Public Radio of Armenia. Archived from the original on 2021-05-13. Retrieved 2021-05-13.\\n\\n^ Park, S.; Sharlin, Ehud; Kitamura, Y.; Lau, E. (29 April 2005). Synthetic Personality in Robots and its Effect on Human-Robot Relationship (Report). doi:10.11575/PRISM/31041. hdl:1880/45619.\\n\\n^ \"Robot Receptionist Dishes Directions and Attitude\". NPR.org. Archived from the original on 2020-12-01. Retrieved 2018-04-05.\\n\\n^ \"New Scientist: A good robot has personality but not looks\" (PDF). Archived from the original (PDF) on 2006-09-29.\\n\\n^ \"Playtime with Pleo, your robotic dinosaur friend\". 25 September 2008. Archived from the original on 2019-01-20. Retrieved 2014-12-14.\\n\\n^ NOVA conversation with Professor Moravec, October 1997. NOVA Online Archived 2017-08-02 at the Wayback Machine\\n\\n^ Agarwal, P.K. Elements of Physics XI. Rastogi Publications. p.\\xa02. ISBN\\xa0978-81-7133-911-2.\\n\\n^ Sandhana, Lakshmi (5 September 2002). \"A Theory of Evolution, for Robots\". Wired. Archived from the original on 2014-03-29. Retrieved 2007-10-28.\\n\\n^ \"Experimental Evolution In Robots Probes The Emergence Of Biological Communication\". Science Daily. 24 February 2007. Archived from the original on 2018-11-16. Retrieved 2007-10-28.\\n\\n^ Žlajpah, Leon (15 December 2008). \"Simulation in robotics\". Mathematics and Computers in Simulation. 79 (4): 879–897. doi:10.1016/j.matcom.2008.02.017.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Žlajpah, Leon (15 December 2008). \"Simulation in robotics\". Mathematics and Computers in Simulation. 79 (4): 879–897. doi:10.1016/j.matcom.2008.02.017.\\n\\n^ \"Evolution trains robot teams TRN 051904\". Technology Research News. Archived from the original on 2016-06-23. Retrieved 2009-01-22.\\n\\n^ Tandon, Prateek (2017). Quantum Robotics. Morgan & Claypool Publishers. ISBN\\xa0978-1627059138.\\n\\n^ Dragani, Rachelle (8 November 2018). \"Can a robot make you a \\'superworker\\'?\". Verizon Communications. Archived from the original on 2020-08-06. Retrieved 2018-12-03.\\n\\n^ \"Robotics\". American Elements. Retrieved 2023-04-10.\\n\\n^ \"Career: Robotics Engineer\". Princeton Review. 2012. Archived from the original on 2015-01-21. Retrieved 2012-01-27.\\n\\n^ Saad, Ashraf; Kroutil, Ryan (2012). Hands-on Learning of Programming Concepts Using Robotics for Middle and High School Students. Proceedings of the 50th Annual Southeast Regional Conference of the Association for Computing Machinery. ACM. pp.\\xa0361–362. doi:10.1145/2184512.2184605.\\n\\n^ Toy, Tommy (29 June 2011). \"Outlook for robotics and Automation for 2011 and beyond are excellent says expert\". PBT Consulting. Archived from the original on 2012-01-27. Retrieved 2012-01-27.\\n\\n^ Frey, Carl Benedikt; Osborne, Michael A. (January 2017). \"The future of employment: How susceptible are jobs to computerisation?\". Technological Forecasting and Social Change. 114: 254–280. CiteSeerX\\xa010.1.1.395.416. doi:10.1016/j.techfore.2016.08.019.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ McGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID\\xa0243172487. SSRN\\xa03044448.\\n\\n^ Hawking, Stephen (1 January 2016). \"This is the most dangerous time for our planet\". The Guardian. Archived from the original on 2021-01-31. Retrieved 2019-11-22.\\n\\n^ \"Robotics – Thematic Research\". GlobalData. Archived from the original on 2021-09-28. Retrieved 2021-09-22.\\n\\n^ \"Focal Points Seminar on review articles in the future of work – Safety and health at work – EU-OSHA\". osha.europa.eu. Archived from the original on 2020-01-25. Retrieved 2016-04-19.\\n\\n^ \"Robotics: Redefining crime prevention, public safety and security\". SourceSecurity.com. Archived from the original on 2017-10-09. Retrieved 2016-09-16.\\n\\n^ \"Draft Standard for Intelligent Assist Devices — Personnel Safety Requirements\" (PDF). Archived (PDF) from the original on 2020-11-25. Retrieved 2016-06-01.\\n\\n^ \"ISO/TS 15066:2016 – Robots and robotic devices – Collaborative robots\". 8 March 2016. Archived from the original on 2016-10-10. Retrieved 2016-06-01.\\n\\n^ Brogårdh, Torgny (January 2007). \"Present and future robot control development—An industrial perspective\". Annual Reviews in Control. 31 (1): 69–79. doi:10.1016/j.arcontrol.2007.01.002. ISSN\\xa01367-5788.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Wang, Tian-Miao; Tao, Yong; Liu, Hui (17 April 2018). \"Current Researches and Future Development Trend of Intelligent Robot: A Review\". International Journal of Automation and Computing. 15 (5): 525–546. doi:10.1007/s11633-018-1115-1. ISSN\\xa01476-8186. S2CID\\xa0126037910. Archived from the original on 2023-03-15. Retrieved 2023-03-15.\\n\\n^ Needham, Joseph (1991). Science and Civilisation in China: Volume 2, History of Scientific Thought. Cambridge University Press. ISBN\\xa0978-0-521-05800-1.\\n\\n^ Fowler, Charles B. (October 1967). \"The Museum of Music: A History of Mechanical Instruments\". Music Educators Journal. 54 (2): 45–49. doi:10.2307/3391092. JSTOR\\xa03391092. S2CID\\xa0190524140.\\n\\n^ Rosheim, Mark E. (1994). Robot Evolution: The Development of Anthrobotics. Wiley-IEEE. pp.\\xa09–10. ISBN\\xa0978-0-471-02622-8.\\n\\n^ al-Jazari (Islamic artist) Archived 2008-05-07 at the Wayback Machine, Encyclopædia Britannica.\\n\\n^ A. P. Yuste. Electrical Engineering Hall of Fame. Early Developments of Wireless Remote Control: The Telekino of Torres-Quevedo,(pdf) vol. 96, No. 1, January 2008, Proceedings of the IEEE.\\n\\n^ H. R. Everett (2015). Unmanned Systems of World Wars I and II. MIT Press. pp.\\xa091–95. ISBN\\xa0978-0-262-02922-3.\\n\\n^ Randy Alfred, \"Nov. 7, 1905: Remote Control Wows Public\", Wired, 7 November 2011.\\n\\n^ Williams, Andrew (16 March 2017). History of Digital Games: Developments in Art, Design and Interaction. CRC Press. ISBN\\xa09781317503811.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ Williams, Andrew (16 March 2017). History of Digital Games: Developments in Art, Design and Interaction. CRC Press. ISBN\\xa09781317503811.\\n\\n^ Randell, Brian (October 1982). \"From Analytical Engine to Electronic Digital Computer: The Contributions of Ludgate, Torres, and Bush\". IEEE Annals of the History of Computing. 4 (4): 327–341. doi:10.1109/MAHC.1982.10042. S2CID\\xa01737953.\\n\\n^ L. Torres Quevedo. Ensayos sobre Automática - Su definicion. Extension teórica de sus aplicaciones, Revista de la Academia de Ciencias Exacta, Revista 12, pp.391-418, 1914.\\n\\n^ Torres Quevedo, Leonardo. Automática: Complemento de la Teoría de las Máquinas, (pdf), pp. 575-583, Revista de Obras Públicas, 19 November 1914.\\n\\n^ L. Torres Quevedo. Essais sur l\\'Automatique - Sa définition. Etendue théorique de ses applications Archived 2023-02-10 at the Wayback Machine, Revue Génerale des Sciences Pures et Appliquées, vol.2, pp.601-611, 1915.\\n\\n^ B. Randell. Essays on Automatics, The Origins of Digital Computers, pp.89-107, 1982.\\n\\n^ PhD, Renato M.E. Sabbatini. \"Sabbatini, RME: An Imitation of Life: The First Robots\". Archived from the original on 2009-07-20. Retrieved 2023-03-15.\\n\\n^ Waurzyniak, Patrick (2006). \"Masters of Manufacturing: Joseph F. Engelberger\". Society of Manufacturing Engineers. 137 (1). Archived from the original on 2011-11-09.\\n\\n^ \"Humanoid History -WABOT-\". www.humanoid.waseda.ac.jp. Archived from the original on 2017-09-01. Retrieved 2017-05-06.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ \"Humanoid History -WABOT-\". www.humanoid.waseda.ac.jp. Archived from the original on 2017-09-01. Retrieved 2017-05-06.\\n\\n^ Zeghloul, Saïd; Laribi, Med Amine; Gazeau, Jean-Pierre (21 September 2015). Robotics and Mechatronics: Proceedings of the 4th IFToMM International Symposium on Robotics and Mechatronics. Springer. ISBN\\xa09783319223681. Archived from the original on 2023-03-15. Retrieved 2017-09-10 – via Google Books.\\n\\n^ \"Historical Android Projects\". androidworld.com. Archived from the original on 2005-11-25. Retrieved 2017-05-06.\\n\\n^ Robots: From Science Fiction to Technological Revolution Archived 2023-03-15 at the Wayback Machine, page 130\\n\\n^ Duffy, Vincent G. (19 April 2016). Handbook of Digital Human Modeling: Research for Applied Ergonomics and Human Factors Engineering. CRC Press. ISBN\\xa09781420063523. Archived from the original on 2023-03-15. Retrieved 2017-09-10 – via Google Books.\\n\\n^ \"KUKA Industrial Robot FAMULUS\". Archived from the original on 2009-02-20. Retrieved 2008-01-10.\\n\\n^ \"History of Industrial Robots\" (PDF). Archived from the original (PDF) on 2012-12-24. Retrieved 2012-10-27.\\n\\n^ R. J. Popplestone; A. P. Ambler; I. Bellos (1978). \"RAPT: A language for describing assemblies\". Industrial Robot. 5 (3): 131–137. doi:10.1108/eb004501.'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='^ R. J. Popplestone; A. P. Ambler; I. Bellos (1978). \"RAPT: A language for describing assemblies\". Industrial Robot. 5 (3): 131–137. doi:10.1108/eb004501.\\n\\n^ Bozinovski, S. (1994). \"Parallel programming for mobile robot control: Agent-based approach\". 14th International Conference on Distributed Computing Systems. pp.\\xa0202–208. doi:10.1109/ICDCS.1994.302412. ISBN\\xa00-8186-5840-1. S2CID\\xa027855786.\\n\\n\\nFurther reading[edit]\\nR. Andrew Russell (1990). Robot Tactile Sensing. New York: Prentice Hall. ISBN\\xa0978-0-13-781592-0.\\nMcGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID\\xa0243172487. SSRN\\xa03044448.\\nAutor, David H. (1 August 2015). \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\". Journal of Economic Perspectives. 29 (3): 3–30. doi:10.1257/jep.29.3.3. hdl:1721.1/109476.\\nTooze, Adam (6 June 2019). \"Democracy and Its Discontents\". The New York Review of Books. Vol.\\xa066, no.\\xa010.\\nExternal links[edit]\\n\\n\\nRobotics  at Wikipedia\\'s sister projects\\n\\nDefinitions from WiktionaryMedia from CommonsTextbooks from WikibooksResources from Wikiversity'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"‹The template Curlie is being considered for deletion.›\\xa0Robotics at Curlie\\nIEEE Robotics and Automation Society\\nInvestigation of social robots – Robots that mimic human behaviors and gestures.\\nWired's guide to the '50 best robots ever', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).\\nvteRoboticsMain articles\\nOutline\\nGlossary\\nIndex\\nHistory\\nGeography\\nHall of Fame\\nEthics\\nLaws\\nCompetitions\\nAI competitions\\nTypes\\nAerobot\\nAnthropomorphic\\nHumanoid\\nAndroid\\nCyborg\\nGynoid\\nClaytronics\\nCompanion\\nAutomaton\\nAnimatronic\\nAudio-Animatronics\\nIndustrial\\nArticulated\\narm\\nDomestic\\nEducational\\nEntertainment\\nJuggling\\nMilitary\\nMedical\\nService\\nDisability\\nAgricultural\\nFood service\\nRetail\\nBEAM robotics\\nSoft robotics\\nClassifications\\nBiorobotics\\nCloud robotics\\nContinuum robot\\nUnmanned vehicle\\naerial\\nground\\nMobile robot\\nMicrobotics\\nNanorobotics\\nNecrobotics\\nRobotic spacecraft\\nSpace probe\\nSwarm\\nTelerobotics\\nUnderwater\\nremotely-operated\\nRobotic fish\\nLocomotion\\nTracks\\nWalking\\nHexapod\\nClimbing\\nElectric unicycle\\nRobotic fins\\nNavigation and mapping\\nMotion planning\\nSimultaneous localization and mapping\\nVisual odometry\\nVision-guided robot systems\\nResearch\\nEvolutionary\\nKits\\nSimulator\\nSuite\\nOpen-source\\nSoftware\\nAdaptable\\nDevelopmental\\nHuman–robot interaction\\nParadigms\\nPerceptual\\nSituated\\nUbiquitous\\nCompanies\\nAmazon Robotics\\nAnybots\\nBarrett Technology\\nBoston Dynamics\\nEnergid Technologies\\nFarmWise\\nFANUC\\nFigure AI\\nFoster-Miller\\nHarvest Automation\\nHoneybee Robotics\\nIntuitive Surgical\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Ubiquitous\\nCompanies\\nAmazon Robotics\\nAnybots\\nBarrett Technology\\nBoston Dynamics\\nEnergid Technologies\\nFarmWise\\nFANUC\\nFigure AI\\nFoster-Miller\\nHarvest Automation\\nHoneybee Robotics\\nIntuitive Surgical\\nIRobot\\nKUKA\\nStarship Technologies\\nSymbotic\\nUniversal Robotics\\nWolf Robotics\\nYaskawa\\nRelated\\nCritique of work\\nPowered exoskeleton\\nWorkplace robotics safety\\nRobotic tech vest\\nTechnological unemployment\\nTerrainability\\nFictional robots'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Category\\n Outline'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"vteEngineering\\nHistory\\nOutline\\nList of engineering branches\\nSpecialtiesandInterdisciplinarityCivil\\nArchitectural\\nCoastal\\nConstruction\\nEarthquake\\nEnvironmental\\nEcological\\nSanitary\\nGeological\\nGeotechnical\\nHydraulic\\nMining\\nMunicipal/Urban\\nOffshore\\nRiver\\nStructural\\nTransportation\\nTraffic\\nRailway\\nMechanical\\nAcoustic\\nAerospace\\nAutomotive\\nBiomechanical\\nEnergy\\nManufacturing\\nMarine\\nNaval architecture\\nRailway\\nSports\\nThermal\\nTribology\\nElectrical\\nBroadcast\\nComputer\\noutline\\nControl\\nElectromechanics\\nElectronics\\nMicrowaves\\nOptical\\nPower\\nRadio frequency\\nSignal processing\\nTelecommunications\\nChemical\\nBiochemical/Bioprocess\\nBiological\\nBioresource\\nGenetic\\nTissue\\nChemical reaction\\nElectrochemical\\nFood\\nMolecular\\nPaper\\nPetroleum\\nProcess\\nReaction\\nMaterials\\nBiomaterial\\nCeramics\\nCorrosion\\nMetallurgy\\nMolecular\\nNanotechnology\\nPolymers\\nSemiconductors\\nSurfaces\\nOther\\nAgricultural\\nAudio\\nAutomation\\nBiomedical\\nBioinformatics\\nClinical\\nHealth technology\\nPharmaceutical\\nRehabilitation\\nBuilding services\\nMEP\\nGeoengineering\\nDesign\\nEngineering drawing/graphics\\nEngineering management\\nEngineering mathematics\\nEngineering physics\\nExplosives\\nFacilities\\nFire\\nForensic\\nGeomatics\\nIndustrial\\nInformation\\nInstrumentation\\nand Control\\nLogistics\\nRobotics\\nMechatronics\\nMilitary\\nNuclear\\nOntology\\nPackaging\\nPrivacy\\nSafety\\nSurvey\\nSecurity\\nSoftware\\nSustainability\\nSystems\\nTextile\\nEngineering education\\nBachelor of Engineering\\nBachelor of Science\\nMaster's degree\\nDoctorate\\nGraduate certificate\\nEngineer's degree\\nLicensed engineer\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Security\\nSoftware\\nSustainability\\nSystems\\nTextile\\nEngineering education\\nBachelor of Engineering\\nBachelor of Science\\nMaster's degree\\nDoctorate\\nGraduate certificate\\nEngineer's degree\\nLicensed engineer\\nRelated topics\\nEngineer\\nGlossaries\\nEngineering\\nA–L\\nM–Z\\nAerospace engineering\\nCivil engineering\\nElectrical and electronics engineering\\nMechanical engineering\\nStructural engineering\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content=\"Category\\n Commons\\n Wikiproject\\n Portal\\n\\nvteEmerging technologiesFieldsManufacturing\\n3D microfabrication\\n3D printing\\n3D publishing\\nClaytronics\\nMolecular assembler\\nSmart manufacturing\\nUtility fog\\nMaterials science\\nAerogel\\nAmorphous metal\\nArtificial muscle\\nConductive polymer\\nFemtotechnology\\nFullerene\\nGraphene\\nHigh-temperature superconductivity\\nHigh-temperature superfluidity\\nLinear acetylenic carbon\\nMetamaterials\\nMetamaterial cloaking\\nMetal foam\\nMulti-function structures\\nNanotechnology\\nCarbon nanotubes\\nMolecular nanotechnology\\nNanomaterials\\nPicotechnology\\nProgrammable matter\\nQuantum dots\\nSilicene\\nSynthetic diamond\\nRobotics\\nDomotics\\nNanorobotics\\nPowered exoskeleton\\nSelf-reconfiguring modular robot\\nSwarm robotics\\nUncrewed vehicle\\nTopics\\nAutomation\\nCollingridge dilemma\\nDifferential technological development\\nDisruptive innovation\\nEphemeralization\\nEthics\\nBioethics\\nCyberethics\\nNeuroethics\\nRobot ethics\\nExploratory engineering\\nProactionary principle\\nTechnological change\\nTechnological unemployment\\nTechnological convergence\\nTechnological evolution\\nTechnological paradigm\\nTechnology forecasting\\nAccelerating change\\nFuture-oriented technology analysis\\nHorizon scanning\\nMoore's law\\nTechnological singularity\\nTechnology scouting\\nTechnology in science fiction\\nTechnology readiness level\\nTechnology roadmap\\nTranshumanism\\n\\n List\"), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='List\\n\\nvteGlossaries of science and engineering\\nAerospace engineering\\nAgriculture\\nArchaeology\\nArchitecture\\nArtificial intelligence\\nAstronomy\\nBiology\\nBotany\\nCalculus\\nCell biology\\nChemistry\\nCivil engineering\\nClinical research\\nComputer hardware\\nComputer science\\nDevelopmental and reproductive biology\\nEcology\\nEconomics\\nElectrical and electronics engineering\\nEngineering\\nA–L\\nM–Z\\nEntomology\\nEnvironmental science\\nGenetics and evolutionary biology\\nCellular and molecular biology\\n0–L\\nM–Z\\nGeography\\nA–M\\nN–Z\\nArabic toponyms\\nHebrew toponyms\\nWestern and South Asia\\nGeology\\nIchthyology\\nMachine vision\\nMathematics\\nMechanical engineering\\nMedicine\\nMeteorology\\nMycology\\nNanotechnology\\nOrnithology\\nPhysics\\nProbability and statistics\\nPsychiatry\\nQuantum computing\\nRobotics\\nScientific naming\\nStructural engineering\\nVirology\\n\\nAuthority control databases NationalGermanyUnited StatesFranceBnF dataCzech RepublicSpainIsraelOtherNARA\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Robotics&oldid=1247477220\"')]\n",
            "Your 1 documents have been split into 97 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Hugging Face model for embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy().tolist()\n",
        "\n",
        "\n",
        "class CustomEmbeddings:\n",
        "    def embed_documents(self, texts):\n",
        "        return [get_embedding(chunk.page_content) for chunk in splits]\n",
        "    def embed_query(self, text):\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Convert the embedding to a list\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy().tolist()\n",
        "\n",
        "\n",
        "embedding_model = CustomEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_QvF6MwkwF5",
        "outputId": "1059003b-c8c0-4e56-cf98-a98dc40cf2da"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "persist_directory = \"db_storage\"\n",
        "\n",
        "db = Chroma.from_documents(splits, embedding_model, persist_directory=persist_directory)\n",
        "db.persist()"
      ],
      "metadata": {
        "id": "6gusk4ZglD1l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.prompts import PromptTemplate\n",
        "import logging"
      ],
      "metadata": {
        "id": "D3JqjgWTmFTR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multiquery\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "y8EqIxskmiAs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Using multiquery vector\n",
        "\n",
        "question = \"When was robotics start getting used in industry \"\n",
        "\n",
        "llm = ChatGroq()\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever = db.as_retriever(), llm = llm\n",
        ")"
      ],
      "metadata": {
        "id": "YLBs_oUUm5uP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs_multi_query = multi_query_retriever.get_relevant_documents(query = question)"
      ],
      "metadata": {
        "id": "r_525DSwn13h"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, dont try to make up an answer.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"input\"])"
      ],
      "metadata": {
        "id": "_LICdb1DnO1W"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm.predict(text=PROMPT.format_prompt(\n",
        "#     context=unique_docs_multi_query,\n",
        "#     question=question\n",
        "# ))\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "document_chain = create_stuff_documents_chain(ChatGroq(), PROMPT)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "response = retrieval_chain.invoke({\"context\": unique_docs_multi_query, \"input\": question})\n"
      ],
      "metadata": {
        "id": "2TJHHNuyoP76"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He7M2hruo52L",
        "outputId": "d65729f6-0b91-4665-c16b-3594d9213532"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales.[5] In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[6]\\nAutonomous transport including airplane autopilot and self-driving cars\\nDomestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading[7] and flatbread baking.[8]\\nConstruction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[9]\\nAutomated mining.\\nSpace exploration, including Mars rovers.\\nEnergy applications including cleanup of nuclear contaminated areas[a]; and cleaning solar panel arrays.\\nMedical robots and Robot-assisted surgery designed and used in clinics.[11]\\nAgricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13]'),\n",
              "  Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales.[5] In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[6]\\nAutonomous transport including airplane autopilot and self-driving cars\\nDomestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading[7] and flatbread baking.[8]\\nConstruction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[9]\\nAutomated mining.\\nSpace exploration, including Mars rovers.\\nEnergy applications including cleanup of nuclear contaminated areas[a]; and cleaning solar panel arrays.\\nMedical robots and Robot-assisted surgery designed and used in clinics.[11]\\nAgricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13]'),\n",
              "  Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.'),\n",
              "  Document(metadata={'source': 'https://en.wikipedia.org/wiki/Robotics'}, page_content='Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.')],\n",
              " 'input': 'When was robotics start getting used in industry ',\n",
              " 'answer': 'Robotics have been increasingly used in manufacturing since the 1960s, according to the provided context. This implies that the use of robotics in industry started around the 1960s and has been growing ever since.'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0zbl1PaTvtsn",
        "outputId": "5784f2e8-93db-4cd8-8302-109265ea0f56"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Robotics have been increasingly used in manufacturing since the 1960s, according to the provided context. This implies that the use of robotics in industry started around the 1960s and has been growing ever since.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Multi vector retrieval\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "import uuid"
      ],
      "metadata": {
        "id": "LdULnfkvv36E"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_chain = load_summarize_chain(llm)"
      ],
      "metadata": {
        "id": "hbdbRTpMxNy1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_key = \"doc_key\"\n",
        "summaries = []\n",
        "for chunk in splits:\n",
        "  unique_id = str(uuid.uuid4())\n",
        "  chunk_summary = summarize_chain.run([chunk])\n",
        "  chunk_summary_document = Document(page_content=chunk_summary, metadata={id_key: unique_id})\n",
        "  summaries.append(chunk_summary_document)\n",
        "  chunk.metadata[id_key] = unique_id"
      ],
      "metadata": {
        "id": "oln2v2r8yBFC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"You have {len(summaries)} summaries to go along with {len(splits)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h7EN8qxyvF8",
        "outputId": "0a76c1c5-bd54-4bce-d582-71763031fa57"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 97 summaries to go along with 97 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory_multi_vector = \"db_storage_multi_vector\"\n",
        "\n",
        "db_multi_vector = Chroma.from_documents(summaries, embedding_model, collection_name=\"summaries\", persist_directory=persist_directory_multi_vector)\n",
        "db_multi_vector.persist()"
      ],
      "metadata": {
        "id": "uURtWBRFzRlL"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KQl5RfV--Mz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docstore_multi_vector = InMemoryStore()"
      ],
      "metadata": {
        "id": "BqbiJfW1zvHo"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector = MultiVectorRetriever(\n",
        "    vectorstore=db_multi_vector,\n",
        "    docstore=docstore_multi_vector,\n",
        "    id_key=id_key\n",
        ")"
      ],
      "metadata": {
        "id": "HpsgH3Tizz4t"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retriever_multi_vector.vectorstore.add_documents(summaries)"
      ],
      "metadata": {
        "id": "o3TAYZIH0K2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _similar_docs = retriever_multi_vector.vectorstore.similarity_search(\n",
        "#     question\n",
        "# )\n",
        "# _similar_docs[0]"
      ],
      "metadata": {
        "id": "CDFxljT_0OHd"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector.docstore.mset([(x.metadata[id_key], x) for x in splits])"
      ],
      "metadata": {
        "id": "sxzN6i8L0iNo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_retrieved_multi_vector = retriever_multi_vector.get_relevant_documents(question)\n",
        "print(docs_retrieved_multi_vector[0].page_content)\n",
        "print(docs_retrieved_multi_vector[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrSP-C1t0_sm",
        "outputId": "a3dca1f8-ebde-4ce7-c2f9-3905f199748b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales.[5] In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[6]\n",
            "Autonomous transport including airplane autopilot and self-driving cars\n",
            "Domestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading[7] and flatbread baking.[8]\n",
            "Construction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[9]\n",
            "Automated mining.\n",
            "Space exploration, including Mars rovers.\n",
            "Energy applications including cleanup of nuclear contaminated areas[a]; and cleaning solar panel arrays.\n",
            "Medical robots and Robot-assisted surgery designed and used in clinics.[11]\n",
            "Agricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13]\n",
            "{'source': 'https://en.wikipedia.org/wiki/Robotics', '99df2dea-518b-4f7f-b6d1-ca8a5247811a': '99df2dea-518b-4f7f-b6d1-ca8a5247811a', 'doc_key': 'a9586ebc-d0d6-4425-b49b-727fe30125e0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "docs_chain = create_stuff_documents_chain(ChatGroq() , PROMPT)\n",
        "retrieval_chain_multi_vector = create_retrieval_chain(retriever_multi_vector, document_chain)\n",
        "# retriever_multi_vector.invoke(question)\n",
        "response = retrieval_chain_multi_vector.invoke({\"context\": docs_retrieved_multi_vector, \"input\": question})"
      ],
      "metadata": {
        "id": "XU-mEj3K3FgD"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "CsU3OFcbFQv7",
        "outputId": "c2f7286e-5a1f-4032-a4b0-15ad7d3f6070"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Robotics have been increasingly used in manufacturing since the 1960s, according to the provided context. The first example given of this is from 2003, with an IBM keyboard manufacturing factory in Texas that was fully automated. However, it's likely that the use of robotics in industry started before 2003 and has been increasing steadily since then. Additionally, the context states that in 2016, the automotive industry was the main customer of industrial robots with 52% of total sales, indicating that the use of robotics in industry was well established by that point.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gi5XVn_G9h0i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2--yBf-9KBPl",
        "outputId": "6e832062-e284-43dc-c040-5874119ad67f"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.15.13)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.13.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.26.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.10.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.25.9)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured) (0.47)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.8.30)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (8.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (24.1)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (5.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: orderly-set==5.2.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured) (5.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140083 sha256=eceb8d42069db1e9bf1ea23ae13ca9064efc0b3604748487752e60b2536f1e5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pi_heif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgdvALAFLrXB",
        "outputId": "d72c1e34-1b00-4791-fda9-32fe00637109"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pi_heif\n",
            "  Downloading pi_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.10/dist-packages (from pi_heif) (10.4.0)\n",
            "Downloading pi_heif-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (984 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m984.8/984.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pi_heif\n",
            "Successfully installed pi_heif-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured_inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_FgOFjcoMTCF",
        "outputId": "1e436275-40a2-45f4-c4fb-da989a4ee7b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured_inference\n",
            "  Downloading unstructured_inference-0.7.37-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting layoutparser (from unstructured_inference)\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting python-multipart (from unstructured_inference)\n",
            "  Downloading python_multipart-0.0.10-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (0.24.7)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (1.26.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (4.10.0.84)\n",
            "Collecting onnx (from unstructured_inference)\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (1.19.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (2.4.1+cu121)\n",
            "Collecting timm (from unstructured_inference)\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (4.44.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured_inference) (3.10.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured_inference) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured_inference) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured_inference) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured_inference) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured_inference) (1.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured_inference) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured_inference) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured_inference) (4.12.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured_inference) (1.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured_inference) (2.1.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured_inference) (10.4.0)\n",
            "Collecting iopath (from layoutparser->unstructured_inference)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser->unstructured_inference)\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image (from layoutparser->unstructured_inference)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured_inference) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured_inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured_inference) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured_inference) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured_inference) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured_inference) (2.8.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->unstructured_inference) (0.19.1+cu121)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->unstructured_inference) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured_inference) (3.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->unstructured_inference) (1.16.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured_inference) (10.0)\n",
            "Collecting portalocker (from iopath->layoutparser->unstructured_inference)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured_inference) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured_inference) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured_inference) (2024.1)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber->layoutparser->unstructured_inference)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured_inference)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber->layoutparser->unstructured_inference) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber->layoutparser->unstructured_inference) (43.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->unstructured_inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->unstructured_inference) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->unstructured_inference) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured_inference) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser->unstructured_inference) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser->unstructured_inference) (2.22)\n",
            "Downloading unstructured_inference-0.7.37-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.10-py3-none-any.whl (22 kB)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=0e35b3b8564524c1375bff64e0dc3344e27d3e56c716accec419418290550a21\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built iopath\n",
            "Installing collected packages: python-multipart, pypdfium2, portalocker, pdf2image, onnx, iopath, pdfminer.six, timm, pdfplumber, layoutparser, unstructured_inference\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20240706\n",
            "    Uninstalling pdfminer.six-20240706:\n",
            "      Successfully uninstalled pdfminer.six-20240706\n",
            "Successfully installed iopath-0.1.10 layoutparser-0.3.4 onnx-1.16.2 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 portalocker-2.10.1 pypdfium2-4.30.0 python-multipart-0.0.10 timm-1.0.9 unstructured_inference-0.7.37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pdfminer"
                ]
              },
              "id": "a076cb865e03405984b4d85db4d2b445"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQEvVTnUNig2",
        "outputId": "d0f27786-b975-4e18-e4d5-7b66dc68f93f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yolox\n",
            "  Downloading yolox-0.3.0.tar.gz (79 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/80.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/80.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.0/80.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yolox) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from yolox) (2.4.1+cu121)\n",
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.10/dist-packages (from yolox) (4.10.0.84)\n",
            "Collecting loguru (from yolox)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from yolox) (0.24.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from yolox) (4.66.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from yolox) (0.19.1+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from yolox) (10.4.0)\n",
            "Collecting thop (from yolox)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting ninja (from yolox)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yolox) (0.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from yolox) (2.17.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yolox) (2.0.8)\n",
            "Collecting onnx==1.8.1 (from yolox)\n",
            "  Downloading onnx-1.8.1.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of yolox to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting yolox\n",
            "  Downloading yolox-0.2.0.tar.gz (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6XHVGjvOQuB",
        "outputId": "cda913c0-dc37-45c6-8824-3ef2caf4bc58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 2s (123 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123605 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt-get install tesseract-ocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrLRPFnVOucm",
        "outputId": "6c034ba8-b976-4531-b957-22b0bf8ebfe7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 3s (1,753 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123635 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install pdf2image\n",
        "!pip install pikepdf\n",
        "!pip install pypdf\n",
        "!pip install google-cloud-vision\n",
        "!pip install effdet\n",
        "# Do not move to constraints.in, otherwise unstructured-inference will not be upgraded\n",
        "# when unstructured library is.\n",
        "!pip install unstructured-inference==0.7.36\n",
        "!pip install unstructured.pytesseract>=0.3.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ytDW-GwCQL-K",
        "outputId": "6868bb07-422c-42df-a66d-257ef80e8232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.16.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n",
            "Collecting pikepdf\n",
            "  Downloading pikepdf-9.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: Pillow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from pikepdf) (10.4.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf) (1.2.14)\n",
            "Requirement already satisfied: lxml>=4.8 in /usr/local/lib/python3.10/dist-packages (from pikepdf) (4.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pikepdf) (24.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pikepdf) (1.16.0)\n",
            "Downloading pikepdf-9.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pikepdf\n",
            "Successfully installed pikepdf-9.2.1\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Collecting google-cloud-vision\n",
            "  Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2024.8.30)\n",
            "Downloading google_cloud_vision-3.7.4-py2.py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.7.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "8da3906bf3114fc2a068ffcc9e626db6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting effdet\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from effdet) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet) (0.19.1+cu121)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet) (1.0.9)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet) (2.0.8)\n",
            "Collecting omegaconf>=2.0 (from effdet)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet) (6.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet) (1.26.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->effdet) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->effdet) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.1->effdet) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->effdet) (10.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.2->effdet) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.1->effdet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.1->effdet) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet) (2024.8.30)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=85757b6b9d4657aef2170022e795f7333536b42aa6808fcaf1f39cd0f7d13ff2\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting unstructured-inference==0.7.36\n",
            "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (0.0.10)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (0.24.7)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (4.10.0.84)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (1.16.2)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (1.19.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (2.4.1+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (1.0.9)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (4.44.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36) (3.10.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36) (1.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.36) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.36) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.36) (4.12.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36) (1.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36) (2.1.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36) (10.4.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36) (0.11.4)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36) (2.8.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.36) (0.19.1+cu121)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.36) (3.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->unstructured-inference==0.7.36) (1.16.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36) (10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser->unstructured-inference==0.7.36) (2.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.36) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.36) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.36) (2024.1)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.36) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.36) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber->layoutparser->unstructured-inference==0.7.36) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber->layoutparser->unstructured-inference==0.7.36) (43.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->unstructured-inference==0.7.36) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->unstructured-inference==0.7.36) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->unstructured-inference==0.7.36) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.36) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser->unstructured-inference==0.7.36) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser->unstructured-inference==0.7.36) (2.22)\n",
            "Downloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export OCR_AGENT=unstructured.partition.utils.ocr_models.tesseract_ocr.OCRAgentTesseract\n"
      ],
      "metadata": {
        "id": "yGC63s6TQ6Yt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PDF Text SPlitters\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json\n",
        "filename = \"pdf.pdf\"\n",
        "\n",
        "elements = partition_pdf(\n",
        "    filename=filename,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "\n",
        "    strategy=\"hi_res\",\n",
        "    infer_table_structure=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "a1WqOxFt4Qhw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIBWt-M6agUF",
        "outputId": "08269df7-8da8-4e4f-8e81-3355e16a2841"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.128)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain_core) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fRy_-bQjSPv",
        "outputId": "6ea4c8f5-2cf8-42bd-d9fd-787df3b6253c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_groq import ChatGroq\n",
        "from typing import Union, List\n",
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "llm = ChatGroq()\n",
        "runnable = obj | llm\n",
        "from pydantic import BaseModel, validator\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "class Sentences(BaseModel):\n",
        "  sentences:  List[str]\n",
        "\n",
        "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvHXcKYB8E6g",
        "outputId": "0d2fe109-063b-4c53-8fcc-4c0156f8041c"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n",
            "<ipython-input-123-595291215feb>:11: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
            "  @validator('sentences', pre=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_propositions(text):\n",
        "    runnable_output = runnable.invoke({'input': text}).content\n",
        "    print(\"Runnable Output:\", runnable_output)  # Debugging output\n",
        "    propositions = extraction_chain.run(runnable_output)\n",
        "    return propositions\n"
      ],
      "metadata": {
        "id": "5WMK6ZeF6_xV"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in elements:\n",
        "  text = e.text\n",
        "  prop = get_propositions(text)\n",
        "  props = [p.sentences for p in prop]\n",
        "  print(props)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-ppDI7ip81_X",
        "outputId": "dbbbd0e8-a1e5-421e-8562-83567f93a1e9"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runnable Output: [\n",
            "\"Sales prediction can be based on product titles and images\",\n",
            "\"Deep learning approaches can be used for sales prediction\",\n",
            "\"Product titles are a factor in sales prediction\",\n",
            "\"Product images are a factor in sales prediction\",\n",
            "\"Deep learning can be applied to product titles\",\n",
            "\"Deep learning can be applied to product images\",\n",
            "\"Deep learning is a type of machine learning\",\n",
            "\"Machine learning can be used for sales prediction\",\n",
            "\"Sales prediction is a use case for deep learning\",\n",
            "\"Sales prediction is a use case for machine learning\"\n",
            "]\n",
            "[['Sales prediction can be based on product titles and images', 'Product titles are a factor in sales prediction', 'Product images are a factor in sales prediction', 'Deep learning can be applied to product titles', 'Deep learning can be applied to product images', 'Deep learning is a type of machine learning', 'Machine learning can be used for sales prediction']]\n",
            "Runnable Output: [\n",
            "\"Haishan Gao is an individual.\",\n",
            "\"Zhaogiang Bai is an individual.\",\n",
            "\"Haishan Gao is associated with the email address hsgao@stanford.edu.\",\n",
            "\"Zhaogiang Bai is associated with the email address baizq871@stanford.edu.\"\n",
            "]\n",
            "[['Haishan Gao is an individual.', 'Zhaogiang Bai is an individual.', 'Haishan Gao is associated with the email address hsgao@stanford.edu.', 'Zhaogiang Bai is associated with the email address baizq871@stanford.edu.']]\n",
            "Runnable Output: [\n",
            "\"Online shopping has become the mainstream shopping method over the past few years.\",\n",
            "\"More local retailers have started their businesses on e-commerce platforms.\",\n",
            "\"Few local retailers can survive on e-commerce platforms due to competitive pressure and entry barriers.\",\n",
            "\"The motivation of this project is to identify product listing strategies that can help retailers raise product sales.\",\n",
            "\"A neural network architecture was built to predict product sales from text, image, and other product listing features.\",\n",
            "\"The VGG16 model was used for image data and the TF-IDF model was used for text data in the architecture.\",\n",
            "\"The accuracy of the model is 73.91%, which is higher than the accuracy of human raters, which is 55.33.\",\n",
            "\"E-commerce platforms have been a mainstream channel for vendors to sell products for a couple of decades.\",\n",
            "\"E-commerce vendors rely more on visual and textual information such as product images and titles due to the non-tactile nature of online products.\",\n",
            "\"This project designed and trained a deep learning model to predict sales using product titles and images.\",\n",
            "\"Deep learning was used to solve the problem of identifying the best strategy for visual and textual presentation.\",\n",
            "\"Machines have the potential to learn the trend behind large e-commerce data that is too large for humans to process.\",\n",
            "\"Traditional sales prediction models use time series analysis to forecast sales.\",\n",
            "\"Recent studies use deep learning models to predict sales and have shown high accuracy.\",\n",
            "\"In this project, the influence of two features on sales was mainly considered: image and text.\",\n",
            "\"Textual information of a product, such as descriptions, can have significant impact on sales.\",\n",
            "\"Seasonal, polite, authoritative, and informative descriptions can lead to higher sales.\",\n",
            "\"Previous research has shown that the influence of product photos on sales is differentiated between women’s and men’s clothing and men’s clothing is more susceptible.\"\n",
            "]\n",
            "[['Online shopping has become the mainstream shopping method over the past few years.', 'More local retailers have started their businesses on e-commerce platforms.', 'Few local retailers can survive on e-commerce platforms due to competitive pressure and entry barriers.', 'The motivation of this project is to identify product listing strategies that can help retailers raise product sales.', 'A neural network architecture was built to predict product sales from text, image, and other product listing features.', 'The VGG16 model was used for image data and the TF-IDF model was used for text data in the architecture.', 'The accuracy of the model is 73.91%, which is higher than the accuracy of human raters, which is 55.33.', 'E-commerce platforms have been a mainstream channel for vendors to sell products for a couple of decades.', 'E-commerce vendors rely more on visual and textual information such as product images and titles due to the non-tactile nature of online products.', 'This project designed and trained a deep learning model to predict sales using product titles and images.', 'Deep learning was used to solve the problem of identifying the best strategy for visual and textual presentation.', 'Machines have the potential to learn the trend behind large e-commerce data that is too large for humans to process.', 'Traditional sales prediction models use time series analysis to forecast sales.', 'Recent studies use deep learning models to predict sales and have shown high accuracy.', 'In this project, the influence of two features on sales was mainly considered: image and text.', 'Textual information of a product, such as descriptions, can have significant impact on sales.', 'Seasonal, polite, authoritative, and informative descriptions can lead to higher sales.', 'Previous research has shown that the influence of product photos on sales is differentiated between women’s and men’s clothing and men’s clothing is more susceptible.']]\n",
            "Runnable Output: [\n",
            "\"The dataset is from Kaggle and is named 'Sales of Summer Clothes in E-commerce Wish'.\",\n",
            "\"The dataset consists of 1.5k products and 43 columns including listings, ratings, and sales performance information.\",\n",
            "\"The two primary features from the dataset being studied are product image and product title.\",\n",
            "\"Additional features selected from the dataset to form metadata features.\",\n",
            "\"An example product listing from the dataset with the keyword 'Summer' is displayed in Figure 1.\",\n",
            "\"The images are read from 'product_picture_url' and the dimension is 200 x 200 x 3.\",\n",
            "\"The resolution of the images can be reduced to 128 x 128 x 3 while ensuring visual details can be observed clearly.\",\n",
            "\"For text data, stop words and non-alphabetic terms are removed, terms are lemmatized using WordNetLemmatizer, and made lowercase.\",\n",
            "\"Aside from images and titles, only features that vendors can personalize are kept, such as price, shipping fees, ads boost, and discount ratio.\",\n",
            "\"Features that vendors cannot customize, such as user rating and origin country, are removed.\",\n",
            "\"'Discount_ratio' is added as a feature to investigate its effectiveness in attracting customers.\",\n",
            "\"The color strings in 'product_color' are converted to their corresponding RGB values.\",\n",
            "\"Random Forest is used to rank the importance of the features excluding image and text data.\"\n",
            "]\n",
            "[[\"The dataset is from Kaggle and is named \\\\'Sales of Summer Clothes in E-commerce Wish\\\\'.\", 'The dataset consists of 1.5k products and 43 columns including listings, ratings, and sales performance information.', 'The two primary features from the dataset being studied are product image and product title.', 'Additional features selected from the dataset to form metadata features.', \"An example product listing from the dataset with the keyword \\\\'Summer\\\\' is displayed in Figure 1.\", \"The images are read from \\\\'product_picture_url\\\\' and the dimension is 200 x 200 x 3.\", 'The resolution of the images can be reduced to 128 x 128 x 3 while ensuring visual details can be observed clearly.', 'For text data, stop words and non-alphabetic terms are removed, terms are lemmatized using WordNetLemmatizer, and made lowercase.', 'Aside from images and titles, only features that vendors can personalize are kept, such as price, shipping fees, ads boost, and discount ratio.', 'Features that vendors cannot customize, such as user rating and origin country, are removed.', \"\\\\'Discount_ratio\\\\' is added as a feature to investigate its effectiveness in attracting customers.\", \"The color strings in \\\\'product_color\\\\' are converted to their corresponding RGB values.\", 'Random Forest is used to rank the importance of the features excluding image and text data.']]\n",
            "Runnable Output: [\n",
            "\"The feature 'countries\\_shipped\\_to' has an importance score of 0.20\",\n",
            "\"The feature 'badges\\_count' has an importance score of 0.03\",\n",
            "\"The feature 'discount\\_ratio' has an importance score of 0.16\",\n",
            "\"The feature 'uses\\_ad\\_boosts' has an importance score of 0.03\",\n",
            "\"The feature 'log(price)' has an importance score of 0.16\",\n",
            "\"The feature 'urgent' has an importance score of 0.02\",\n",
            "\"The feature 'log(retail\\_price)' has an importance score of 0.16\",\n",
            "\"The feature 'badge\\_fast\\_shipping' has an importance score of 0.01\",\n",
            "\"The feature 'shipping\\_option\\_price' has an importance score of 0.06\",\n",
            "\"The feature 'badge\\_local\\_product' has an importance score of 0.01\",\n",
            "\"The feature 'product\\_color' has an importance score of 0.06\",\n",
            "\"The feature 'inventory\\_total' has an importance score of 0.00\",\n",
            "\"The feature 'badge\\_product\\_quality' has an importance score of 0.03\",\n",
            "\"The feature 'shipping\\_is\\_express' has an importance score of 0.00\"\n",
            "]\n",
            "[[\"The feature \\\\'countries_shipped_to\\\\' has an importance score of 0.20\", \"The feature \\\\'badges_count\\\\' has an importance score of 0.03\", \"The feature \\\\'discount_ratio\\\\' has an importance score of 0.16\", \"The feature \\\\'uses_ad_boosts\\\\' has an importance score of 0.03\", \"The feature \\\\'log(price)\\\\' has an importance score of 0.16\", \"The feature \\\\'urgent\\\\' has an importance score of 0.02\", \"The feature \\\\'log(retail_price)\\\\' has an importance score of 0.16\", \"The feature \\\\'badge_fast_shipping\\\\' has an importance score of 0.01\", \"The feature \\\\'shipping_option_price\\\\' has an importance score of 0.06\", \"The feature \\\\'badge_local_product\\\\' has an importance score of 0.01\", \"The feature \\\\'product_color\\\\' has an importance score of 0.06\", \"The feature \\\\'inventory_total\\\\' has an importance score of 0.00\", \"The feature \\\\'badge_product_quality\\\\' has an importance score of 0.03\", \"The feature \\\\'shipping_is_express\\\\' has an importance score of 0.00\"]]\n",
            "Runnable Output: [\n",
            "\"The sales information 'unit\\_sold' in the dataset is obtained by scraping displayed sentences 'N bought this' on the product listing page.\",\n",
            "\"We label the dataset with binary values 'high sales' and 'low sales' based on the value of 'unit\\_sold'.\",\n",
            "\"We define the sales prediction problem as binary classification for the purpose of selecting the optimal architecture of image and text models.\",\n",
            "\"'Sales Performance Histogram' is a histogram that shows the distribution of 'unit\\_sold'.\",\n",
            "\"In our features baseline model, we use the 16 selected features only as the input and Random Forest as the prediction model.\",\n",
            "\"The average accuracy of the features baseline model is about 0.6540.\",\n",
            "\"We set up image and text baseline models to study the relative impacts of images and texts.\",\n",
            "\"The baseline image model has a simple CNN structure of three layers of [CONV-16, DROPOUT-0.3, MAXPOOLING] followed by a FLATTEN layer and a dense layer with RELU activation function.\",\n",
            "\"The result of the baseline image model has relatively low bias with 0.84 train accuracy but high variance with 0.61 dev accuracy.\",\n",
            "\"The baseline text model uses TF-IDF to encode text data and the network architecture is CONV-24, DROPOUT, MAXPOOLING followed by a FLATTEN layer and then a dense layer with sigmoid as the final output for prediction class.\",\n",
            "\"We observe 0.84 train accuracy and 0.67 dev accuracy in the baseline text model.\",\n",
            "\"We use VGG-16 network in Keras, which is a pre-trained version on more than ten million images from the ImageNet database, for image models.\",\n",
            "\"We apply transfer learning and keep all the weights of the convolutional blocks frozen in the VGG-16 network.\",\n",
            "\"Only the parameters in the fully-connected blocks were trained on our own dataset in the VGG-16 network.\",\n",
            "\"The original VGG-16 output layer of 1000 classes is modified to just 2 classes for our binary classification task in the VGG-16 network.\",\n",
            "\"In comparison with the image baseline model, the VGG-16 network improves the dev accuracy from 0.6022 to 0.6497.\",\n",
            "\"We regard 0.65 accuracy acceptable in the VGG-16 network.\",\n",
            "\"The VGG16 model has lower training accuracy than the baseline CNN, which we attribute to the fact that the VGG networks employ regularization techniques like dropouts.\",\n",
            "\"We apply image augmentation technique to artificially expand the size of our training dataset to mitigate the problems that may arise from limited dataset size.\",\n",
            "\"50,000 images are generated through manipulations such as shifts, flips, crops, rotation, and zooms.\",\n",
            "\"We use the the augmented dataset to train and validate our models, but the training and validation accuracy is not improved.\"\n",
            "]\n",
            "[[\"The sales information \\\\'unit_sold\\\\' in the dataset is obtained by scraping displayed sentences \\\\'N bought this\\\\' on the product listing page.\", \"We label the dataset with binary values \\\\'high sales\\\\' and \\\\'low sales\\\\' based on the value of \\\\'unit_sold\\\\'.\", 'We define the sales prediction problem as binary classification for the purpose of selecting the optimal architecture of image and text models.', \"\\\\'Sales Performance Histogram\\\\' is a histogram that shows the distribution of \\\\'unit_sold\\\\'.\", 'In our features baseline model, we use the 16 selected features only as the input and Random Forest as the prediction model.', 'The average accuracy of the features baseline model is about 0.6540.', 'We set up image and text baseline models to study the relative impacts of images and texts.', 'The baseline image model has a simple CNN structure of three layers of [CONV-16, DROPOUT-0.3, MAXPOOLING] followed by a FLATTEN layer and a dense layer with RELU activation function.', 'The result of the baseline image model has relatively low bias with 0.84 train accuracy but high variance with 0.61 dev accuracy.', 'The baseline text model uses TF-IDF to encode text data and the network architecture is CONV-24, DROPOUT, MAXPOOLING followed by a FLATTEN layer and then a dense layer with sigmoid as the final output for prediction class.', 'We observe 0.84 train accuracy and 0.67 dev accuracy in the baseline text model.', 'We use VGG-16 network in Keras, which is a pre-trained version on more than ten million images from the ImageNet database, for image models.', 'We apply transfer learning and keep all the weights of the convolutional blocks frozen in the VGG-16 network.', 'Only the parameters in the fully-connected blocks were trained on our own dataset in the VGG-16 network.', 'The original VGG-16 output layer of 1000 classes is modified to just 2 classes for our binary classification task in the VGG-16 network.', 'In comparison with the image baseline model, the VGG-16 network improves the dev accuracy from 0.6022 to 0.6497.', 'We regard 0.65 accuracy acceptable in the VGG-16 network.', 'The VGG16 model has lower training accuracy than the baseline CNN, which we attribute to the fact that the VGG networks employ regularization techniques like dropouts.', 'We apply image augmentation technique to artificially expand the size of our training dataset to mitigate the problems that may arise from limited dataset size.', '50,000 images are generated through manipulations such as shifts, flips, crops, rotation, and zooms.', 'We use the the augmented dataset to train and validate our models, but the training and validation accuracy is not improved.']]\n",
            "Runnable Output: [\n",
            "\"The Doc2Vec model was used to represent each title as a vector.\",\n",
            "\"Including features with the title input improved the model accuracy significantly.\",\n",
            "\"Using only title information as input had limited performance.\",\n",
            "\"The second model used TF-IDF as the embedding of text sentences.\",\n",
            "\"The TF-IDF modification increased the validation accuracy with the same model architecture.\",\n",
            "\"Sentence Transformers presented worse performance than using TF-IDF models.\",\n",
            "\"Easy Data Augmentation was used to augment the number of labeled data.\",\n",
            "\"Four ways were introduced to augment the data set: replacing synonyms, inserting synonyms, swapping words, and removing words.\",\n",
            "\"These methods increased the training data from 1,537 to 15,731.\",\n",
            "\"Increasing the amount of training data didn't improve the performance of a logistic regression binary classifier or a neural network.\",\n",
            "\"The best model performance with augmented training data stayed around 72% dev accuracy.\",\n",
            "\"Product tags were used to help the training process.\",\n",
            "\"Adding tag information didn't improve model performance and slowed down the training process due to a larger input space.\",\n",
            "\"A TF-IDF model was used to encode titles and pass the embeddings into a logistic regression classifier to make the prediction.\",\n",
            "\"The output probabilities were concatenated later with other features as the next level model input.\",\n",
            "\"This change made the model performance a bit worse than just using TF-IDF embedding with features as input.\",\n",
            "\"One possibility is there isn't enough data to build independent components for any ensemble methods.\"\n",
            "]\n",
            "[['The Doc2Vec model was used to represent each title as a vector.', 'Including features with the title input improved the model accuracy significantly.', 'Using only title information as input had limited performance.', 'The second model used TF-IDF as the embedding of text sentences.', 'The TF-IDF modification increased the validation accuracy with the same model architecture.', 'Sentence Transformers presented worse performance than using TF-IDF models.', 'Easy Data Augmentation was used to augment the number of labeled data.', 'Four ways were introduced to augment the data set: replacing synonyms, inserting synonyms, swapping words, and removing words.', 'These methods increased the training data from 1,537 to 15,731.', \"Increasing the amount of training data didn\\\\'t improve the performance of a logistic regression binary classifier or a neural network.\", 'The best model performance with augmented training data stayed around 72% dev accuracy.', 'Product tags were used to help the training process.', \"Adding tag information didn\\\\'t improve model performance and slowed down the training process due to a larger input space.\", 'A TF-IDF model was used to encode titles and pass the embeddings into a logistic regression classifier to make the prediction.', 'The output probabilities were concatenated later with other features as the next level model input.', 'This change made the model performance a bit worse than just using TF-IDF embedding with features as input.', \"One possibility is there isn\\\\'t enough data to build independent components for any ensemble methods.\"]]\n",
            "Runnable Output: [\n",
            "\"A combined model is created after exploring each modality of the model.\",\n",
            "\"For image data, a transfer learning approach is used with a pre-trained VGG-16 model.\",\n",
            "\"Two [RELU, BATCHNORM, DROPOUT] blocks follow the fully-connected FLATTEN layer for image data, resulting in a 128 by 1 vector.\",\n",
            "\"A TF-IDF model is used for text representation, transforming the title of each example into a 1169 by 1 vector.\",\n",
            "\"Two [CONV1D, MAXPOOLING, DROUPOUT] blocks followed by a FLATTEN layer are used for text data, and the output is passed to a dense layer with RELU activation function, resulting in a 128 by 1 vector.\",\n",
            "\"The image and text vectors are concatenated with the metadata feature vector, and passed through three [RELU, BATCHNORM] layers and a dense layer with sigmoid for binary classification.\",\n",
            "\"The Adam optimizer is used for training with default parameters: Ir = 0.001, β1 = 0.9, β2 = 0.999, ε = 10−7.\",\n",
            "\"The input image resolution to the pre-trained VGG-16 model is (224 × 224 × 3).\",\n",
            "\"Dropout rates are set to 0.2 for all layers.\",\n",
            "\"The number of fully connected layers that train the concatenated vector is 3.\",\n",
            "\"Batch\\_size = 16 is used for training to ensure maximized GPU efficiency and convergence.\",\n",
            "\"The train accuracy is 97.83% and the validation accuracy is 73.91%.\",\n",
            "\"The model is overfitting the training set, as the validation loss does not decrease and the validation accuracy does not increase after certain epochs.\",\n",
            "\"Various regularization approaches have been explored, including L2 norm on weights, data augmentation, and higher dropout rates, but the validation accuracy has not improved.\",\n",
            "\"The hypothesis is that the current accuracy, 73.91%, is a bottleneck for the defined problem.\"\n",
            "]\n",
            "[['For image data, a transfer learning approach is used with a pre-trained VGG-16 model.', 'Two [RELU, BATCHNORM, DROPOUT] blocks follow the fully-connected FLATTEN layer for image data, resulting in a 128 by 1 vector.', 'A TF-IDF model is used for text representation, transforming the title of each example into a 1169 by 1 vector.', 'Two [CONV1D, MAXPOOLING, DROUPOUT] blocks followed by a FLATTEN layer are used for text data, and the output is passed to a dense layer with RELU activation function, resulting in a 128 by 1 vector.', 'The image and text vectors are concatenated with the metadata feature vector, and passed through three [RELU, BATCHNORM] layers and a dense layer with sigmoid for binary classification.']]\n",
            "Runnable Output: [\n",
            "\"The 'Models' refers to different machine learning or deep learning models used in a study.\",\n",
            "\"A 'Random Forest' model has a train accuracy of 0.6540 and a dev accuracy of 0.6142 when using the 'Image' as a baseline.\",\n",
            "\"A 'Simple CNN' model has a train accuracy of 0.8338 and a dev accuracy of 0.6142 when using the 'Image' as a feature.\",\n",
            "\"A 'VGG-16' model has a train accuracy of 0.6022 and a dev accuracy of 0.6497 when using the 'Text' as a baseline.\",\n",
            "\"A 'TF-IDF' model has a train accuracy of 0.8372 and a dev accuracy of 0.6701 when using the 'Text' and 'Features' as features.\",\n",
            "\"A 'Doc2vec' model has a train accuracy of 0.6811 and a dev accuracy of 0.6396 when using the 'Text' and 'Features' as features.\",\n",
            "\"A 'TF-IDF' model has a train accuracy of 0.7693 and a dev accuracy of 0.7259 when using the 'Text' and 'Features' as features.\",\n",
            "\"A 'Sentence Transformers' model has a train accuracy of 0.6716 and a dev accuracy of 0.6333 when using the 'Combined' features.\",\n",
            "\"A 'VGG-16' and 'TF-IDF' model has a train accuracy of 0.9783 and a dev accuracy of 0.7391 when using the 'Text' and 'Features' as features.\"\n",
            "]\n",
            "[[\"The \\\\'Models\\\\' refers to different machine learning or deep learning models used in a study.\", \"A \\\\'Random Forest\\\\' model has a train accuracy of 0.6540 and a dev accuracy of 0.6142 when using the \\\\'Image\\\\' as a baseline.\", \"A \\\\'Simple CNN\\\\' model has a train accuracy of 0.8338 and a dev accuracy of 0.6142 when using the \\\\'Image\\\\' as a feature.\", \"A \\\\'VGG-16\\\\' model has a train accuracy of 0.6022 and a dev accuracy of 0.6497 when using the \\\\'Text\\\\' as a baseline.\", \"A \\\\'TF-IDF\\\\' model has a train accuracy of 0.8372 and a dev accuracy of 0.6701 when using the \\\\'Text\\\\' and \\\\'Features\\\\' as features.\", \"A \\\\'Doc2vec\\\\' model has a train accuracy of 0.6811 and a dev accuracy of 0.6396 when using the \\\\'Text\\\\' and \\\\'Features\\\\' as features.\", \"A \\\\'TF-IDF\\\\' model has a train accuracy of 0.7693 and a dev accuracy of 0.7259 when using the \\\\'Text\\\\' and \\\\'Features\\\\' as features.\", \"A \\\\'Sentence Transformers\\\\' model has a train accuracy of 0.6716 and a dev accuracy of 0.6333 when using the \\\\'Combined\\\\' features.\", \"A \\\\'VGG-16\\\\' and \\\\'TF-IDF\\\\' model has a train accuracy of 0.9783 and a dev accuracy of 0.7391 when using the \\\\'Text\\\\' and \\\\'Features\\\\' as features.\"]]\n",
            "Runnable Output: [\n",
            "\"Human rating has an accuracy of 55.33% in our experiment.\",\n",
            "\"The human rating was performed by randomly selecting around 200 samples and using product description and pictures.\",\n",
            "\"Human raters were instructed to pick around 60% of the products as those they would like to buy and filter out the other 40%.\",\n",
            "\"The experiment showed that the models outperformed human raters in product selection.\",\n",
            "\"Models with text input have better performance in general according to the text feature analysis.\",\n",
            "\"TF-IDF embedding works better than contextual embedding for models with text input.\",\n",
            "\"Product titles have an effect on product sales according to the text feature analysis.\",\n",
            "\"High-sale product titles have more phrases that describe the characteristics of the product or are associated with women's apparel.\",\n",
            "\"Low sales product titles tend to have more terms related to sports or functionalities.\",\n",
            "\"High sales product titles have slightly longer lengths in general.\",\n",
            "\"There is no evidence showing that title length impacts sales directly.\",\n",
            "\"Models with only image as input don't perform as well as those with text input according to the image feature analysis.\",\n",
            "\"The VGG model used for image input is pre-trained on the ImageNet dataset, which is designed for significantly different tasks than commercial products classification and analysis.\",\n",
            "\"The correlation between product sales and images is weak according to the image feature analysis.\",\n",
            "\"Text input is more important than image input or other features for product sales according to the conclusion.\",\n",
            "\"The numerical statistic embedding method TF-IDF performs better than contextual embedding according to the conclusion.\",\n",
            "\"Including specific keywords in the product title can increase the product sales according to the conclusion.\",\n",
            "\"Most e-commerce platforms retrieve products mainly based on keyword matching according to the conclusion.\",\n",
            "\"The Bayes error rate remains unknown for our project due to the inherent difficulty of this task.\",\n",
            "\"The limited improvement of data augmentation on the project might not attribute to the robustness of the model but to the fact it’s reaching the Bayes error rate according to the conclusion.\",\n",
            "\"Potential directions for further exploration include collecting more data for training and using an image network model pre-trained with commercial product images instead of ImageNet according to the conclusion.\"\n",
            "]\n",
            "[['Human rating has an accuracy of 55.33% in our experiment.', 'The human rating was performed by randomly selecting around 200 samples and using product description and pictures.', 'Human raters were instructed to pick around 60% of the products as those they would like to buy and filter out the other 40%.', 'The experiment showed that the models outperformed human raters in product selection.', 'Models with text input have better performance in general according to the text feature analysis.', 'TF-IDF embedding works better than contextual embedding for models with text input.', 'Product titles have an effect on product sales according to the text feature analysis.', \"High-sale product titles have more phrases that describe the characteristics of the product or are associated with women\\\\'s apparel.\", 'Low sales product titles tend to have more terms related to sports or functionalities.', 'High sales product titles have slightly longer lengths in general.', 'There is no evidence showing that title length impacts sales directly.', \"Models with only image as input don\\\\'t perform as well as those with text input according to the image feature analysis.\", 'The VGG model used for image input is pre-trained on the ImageNet dataset, which is designed for significantly different tasks than commercial products classification and analysis.', 'The correlation between product sales and images is weak according to the image feature analysis.', 'Text input is more important than image input or other features for product sales according to the conclusion.', 'The numerical statistic embedding method TF-IDF performs better than contextual embedding according to the conclusion.', 'Including specific keywords in the product title can increase the product sales according to the conclusion.', 'Most e-commerce platforms retrieve products mainly based on keyword matching according to the conclusion.']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-9be9b3857bb8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_propositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-101-6415c16cf926>\u001b[0m in \u001b[0;36mget_propositions\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_propositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrunnable_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Runnable Output:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunnable_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpropositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextraction_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpropositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         return cast(\n\u001b[1;32m    283\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    783\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         flattened_outputs = [\n\u001b[1;32m    643\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 results.append(\n\u001b[0;32m--> 631\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    632\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    854\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         }\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1025\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         return self._request(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Multivector with Proposition based retrieval\n",
        "\n",
        "from pydantic import BaseModel, validator\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "filename = \"pdf.pdf\"\n",
        "elements = partition_pdf(\n",
        "    filename=filename,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    strategy=\"hi_res\",\n",
        "    infer_table_structure=True\n",
        ")\n",
        "\n",
        "# Set up the language model and extraction chain\n",
        "llm = ChatGroq()\n",
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "runnable = obj | llm\n",
        "\n",
        "class Sentences(BaseModel):\n",
        "  sentences:  List[str]\n",
        "\n",
        "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)\n",
        "def get_propositions(text):\n",
        "    runnable_output = runnable.invoke({'input': text}).content\n",
        "    propositions = extraction_chain.run(runnable_output)\n",
        "    return propositions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKAuA8Ra0Ok9",
        "outputId": "d9f8b540-e45c-4367-d2b6-56df0e26c07d"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "id_key = \"doc_key\"\n",
        "docstore_elements = []\n",
        "vectorstore_propositions = []\n",
        "\n",
        "for e in elements:\n",
        "    text = e.text\n",
        "    # Get propositions from the text\n",
        "    prop = get_propositions(text)\n",
        "    propositions = [p.sentences for p in prop]\n",
        "\n",
        "    # Create a unique ID for the document\n",
        "    unique_id = str(uuid.uuid4())\n",
        "\n",
        "    # Store PDF elements in document store\n",
        "    docstore_elements.append(Document(page_content=text, metadata={id_key: unique_id}))\n",
        "\n",
        "    # Store each proposition in the vector store\n",
        "    for sentence in propositions:\n",
        "        for s in sentence:  # Ensure we iterate through the list of sentences\n",
        "            chunk_summary_document = Document(page_content=s, metadata={id_key: unique_id})\n",
        "            vectorstore_propositions.append(chunk_summary_document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "XVh6gNyt6REZ",
        "outputId": "b596d92b-368e-485d-f132-0d02d1756795"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-9b5a46cd27f8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Get propositions from the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_propositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpropositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-ddac1316bed9>\u001b[0m in \u001b[0;36mget_propositions\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mextraction_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_extraction_chain_pydantic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydantic_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_propositions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mrunnable_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mpropositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextraction_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpropositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         return cast(\n\u001b[1;32m    283\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    285\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    783\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         flattened_outputs = [\n\u001b[1;32m    643\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 results.append(\n\u001b[0;32m--> 631\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    632\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    854\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         }\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1025\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1074\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    973\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    955\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    114\u001b[0m                 trace.return_value = (\n\u001b[1;32m    115\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory_multi_vector = \"db_storage_multi_vector\"\n",
        "\n",
        "# Create and persist the vector store\n",
        "db_multi_vector = Chroma.from_documents(vectorstore_propositions, embedding_model, collection_name=\"propositions\", persist_directory=persist_directory_multi_vector)\n",
        "db_multi_vector.persist()\n",
        "\n",
        "# Create and persist the document store\n",
        "# docstore_multi_vector = Chroma.from_documents(docstore_elements, embedding_model, collection_name=\"elements\", persist_directory=\"db_storage_docstore\")\n",
        "# docstore_multi_vector.persist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XzGg5ZcM7khI",
        "outputId": "2d77a6a9-f162-48b1-e603-238aa3c3416f"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'InMemoryStore' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-ba19afc34afe>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create and persist the document store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdocstore_multi_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInMemoryStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# docstore_multi_vector = Chroma.from_documents(docstore_elements, embedding_model, collection_name=\"elements\", persist_directory=\"db_storage_docstore\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# docstore_multi_vector.persist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'InMemoryStore' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "docstore_multi_vector = InMemoryStore()\n",
        "\n",
        "retriever_multi_vector = MultiVectorRetriever(\n",
        "    vectorstore=db_multi_vector,\n",
        "    docstore=docstore_multi_vector,\n",
        "    id_key=id_key\n",
        ")\n",
        "\n",
        "# Link documents in the docstore\n",
        "retriever_multi_vector.docstore.mset([(doc.metadata[id_key], doc) for doc in docstore_elements])\n"
      ],
      "metadata": {
        "id": "PYB2dDtH8nLP"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever = retriever_multi_vector\n",
        ")"
      ],
      "metadata": {
        "id": "a_t8YJX48oE3"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What is the pdf about\""
      ],
      "metadata": {
        "id": "Nmy7xc8XAfRR"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "\n",
        "docs_chain = create_stuff_documents_chain(ChatGroq() , PROMPT)\n",
        "docs_retrieved_multi_vector = compression_retriever.base_retriever.get_relevant_documents(question)\n",
        "# compressor.compress_documents(documents=docs_retrieved_multi_vector, query=question)\n",
        "\n",
        "# retrieval_chain_multi_vector = create_retrieval_chain(retriever_multi_vector, document_chain)\n",
        "# retriever_multi_vector.invoke(question)\n",
        "# response = retrieval_chain_multi_vector.invoke({\"context\": docs_retrieved_multi_vector, \"input\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji3P0a5M_yHI",
        "outputId": "9b5fd743-5f07-470d-f914-58f56a1445ad"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-169-eb173806fe23>:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs_retrieved_multi_vector = compression_retriever.base_retriever.get_relevant_documents(question)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n"
      ],
      "metadata": {
        "id": "bqZRHeGtAo4K"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\"\n",
        "Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, dont try to make up an answer.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"input\"])"
      ],
      "metadata": {
        "id": "Bj3XfSsSA98n"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "retrieval_chain_compressed = create_retrieval_chain(retriever_multi_vector, docs_chain)\n",
        "response = retrieval_chain_compressed.invoke({\"context\": retrieval_chain_compressed, \"input\": question})"
      ],
      "metadata": {
        "id": "_gdA2pGiA-2R"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Msy4p0qPBZDM",
        "outputId": "93ad70f3-4951-4411-e4eb-f90d31c8b197"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The PDF appears to be about a analysis or study on sales prediction using deep learning approaches. The feature importance listed in the context suggests that various product attributes, such as countries shipped to, badges count, discount ratio, use of ad boosts, price, retail price, shipping options, product color, and inventory total, were considered in the predictive model. However, without the full document, this is a general interpretation based on the provided context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pyIUMDmBh3G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}